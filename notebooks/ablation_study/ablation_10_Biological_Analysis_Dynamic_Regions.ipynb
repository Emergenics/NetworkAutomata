{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20939bf3",
   "metadata": {},
   "source": [
    "# Cell 0: Notebook Header & Documentation\n",
    "# Description: Provides context and instructions for this notebook.\n",
    "\n",
    "## Notebook Title: Ablation Study - Biological Analysis of Dynamic Regions\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "*   **Goal:** To perform functional enrichment analysis on the **dynamically identified regions** for *each* simulation run in the Ablation Study (`ablation_01` through `ablation_07`). This goes beyond analyzing only the static Red Region from the baseline run and assesses the biological relevance of the sustained dynamic activity patterns generated by different ruleset variations.\n",
    "*   **Contribution:** Loads the lists of dynamic region node IDs (as calculated and saved by `ablation_09`). Maps these IDs to gene symbols. Performs GO Biological Process enrichment analysis for the dynamic region of each run. Compiles and presents the enrichment findings in a comparative table.\n",
    "*   **Inputs:**\n",
    "    *   Requires the baseline configuration file (`baseline_config.json`) saved by `ablation_00`.\n",
    "    *   Requires the text files containing the dynamic region node IDs for each run (`dynamic_region_nodes_run_label.txt`) generated by `ablation_09`.\n",
    "*   **Outputs:**\n",
    "    *   A dedicated analysis output folder (`biological_analysis_results/Dynamic_Biological_Analysis`).\n",
    "    *   Saved full enrichment results CSVs for each run's dynamic region (e.g., `dynamic_region_run_label_enrichment_results.csv`).\n",
    "    *   A summary CSV file containing the comparative enrichment metrics table across all runs.\n",
    "    *   A markdown summary of the biological analysis findings for dynamic regions.\n",
    "\n",
    "### How to Run\n",
    "\n",
    "*   **Prerequisites:** Ensure `ablation_00_Setup_and_Definitions.ipynb` and ALL simulation notebooks (`ablation_01` through `ablation_07`) have been run successfully. Ensure `ablation_09_Dynamic_Analysis_Across_Runs.ipynb` has been run successfully to calculate and save the dynamic region node lists. Ensure the Canonical Helper Functions for biological analysis are defined in Cell 2 of THIS notebook.\n",
    "*   **Configuration:** No user edits are required; Cell 1 loads the baseline config and defines specific analysis parameters. Cell 3 defines the mapping from analysis labels to run folders (matching `ablation_09`).\n",
    "*   **Execution:** Run all cells sequentially from top to bottom (Cell 0 through Cell 8).\n",
    "*   **Expected Runtime:** High, depends on the number of runs, size of dynamic regions, and the responsiveness of the STRING API and Enrichr server. This step can take significant time due to multiple API calls and enrichment analyses.\n",
    "\n",
    "### Expected Results & Analysis (within this notebook)\n",
    "\n",
    "*   This notebook loads the dynamic region node lists for each of the 7 simulation runs.\n",
    "*   It maps the node IDs to gene symbols and performs GO BP enrichment analysis for each run's dynamic region gene list.\n",
    "*   It compiles a table summarizing the enrichment results (e.g., number of significant terms, top term, its p-value) for the dynamic region of each run.\n",
    "*   A final markdown summary discusses the biological relevance of the dynamic patterns generated by different ruleset variants based on these enrichment results. This table will be used for the final paper draft synthesis in `ablation_11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d9a4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 1: Load Configuration and Define Analysis Parameters (2025-04-28 21:18:25) ---\n",
      "  ✅ Loaded baseline configuration from: simulation_results/Ablation_Setup_Files/baseline_config.json\n",
      "  Base Experiment Name loaded and set globally: string_ca_subgraph_AIFM1_CORRECTED\n",
      "\n",
      "--- Consistent Dynamic Region Analysis Parameters ---\n",
      "  Window: Last 20% of steps\n",
      "  Metric: Time-Avg Abs Change (|Act_t+1-Act_t|, |Inh_t+1-Inh_t|)\n",
      "  Threshold: Above 80th percentile of metric values across nodes\n",
      "\n",
      "Analysis outputs will be saved in: /home/irbsurfer/Projects/Novyte/Emergenics/production/emergenics/1_NetworkIStheComputation/ablation_study/biological_analysis_results/Dynamic_Biological_Analysis\n",
      "   ✅ Saved analysis configuration to biological_analysis_results/Dynamic_Biological_Analysis/dynamic_biological_analysis_config.json\n",
      "\n",
      "Cell 1: Configuration loaded and analysis parameters defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load Configuration and Define Analysis Parameters\n",
    "# Description: Loads the baseline configuration to get directory paths and\n",
    "#              defines the parameters for the consistent dynamic region analysis.\n",
    "#              MODIFIED: Ensures BASE_EXPERIMENT_NAME, TARGET_NODE_ID, TARGET_NODE_NAME,\n",
    "#              API/Enrichment params, etc. are set as globals.\n",
    "#              MODIFIED: Standardized output directory global name to OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS.\n",
    "#              FIXED: Corrected the check for OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS's existence.\n",
    "#              FIXED: Defined INPUT_DIR_DYNAMIC_REGIONS before attempting to use it in the config dict.\n",
    "#              FIXED: Ensure all necessary API/Enrichment parameters are extracted from baseline_config\n",
    "#                     and set as globals before being used in the analysis_config_dict.\n",
    "#              FIXED: Ensure BASE_EXPERIMENT_NAME is set as a global in the outer scope of Cell 1.\n",
    "#              FIXED: Ensure SPECIES_ID and other API/Enrichment parameters are extracted and set as globals.\n",
    "#              FIXED: Ensure OUTPUT_DIR_DYNAMIC_ANALYSIS is set as a global variable.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "import numpy as np # Needed for np.nan checks later\n",
    "\n",
    "print(f\"\\n--- Cell 1: Load Configuration and Define Analysis Parameters ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Load Baseline Configuration ---\n",
    "config_load_error = False\n",
    "baseline_config = {}\n",
    "setup_output_dir_load = os.path.join(\"simulation_results\", \"Ablation_Setup_Files\")\n",
    "\n",
    "try:\n",
    "    config_path_load = os.path.join(setup_output_dir_load, \"baseline_config.json\")\n",
    "    if not os.path.exists(config_path_load): raise FileNotFoundError(f\"Baseline config file not found: {config_path_load}. Run ablation_00.\")\n",
    "    with open(config_path_load, 'r') as f: baseline_config = json.load(f)\n",
    "    print(f\"  ✅ Loaded baseline configuration from: {config_path_load}\")\n",
    "\n",
    "    # Extract needed base parameters and set as GLOBALS\n",
    "    OUTPUT_DIR_SIMULATIONS = baseline_config.get('OUTPUT_DIR', \"simulation_results\") # Base dir for sim outputs\n",
    "    ANALYSIS_DIR_BASE = baseline_config.get('ANALYSIS_DIR', \"biological_analysis_results\") # Base dir for analysis outputs\n",
    "\n",
    "    # --- Extract BASE_EXPERIMENT_NAME and set as GLOBAL ---\n",
    "    # Set BASE_EXPERIMENT_NAME as a global directly in the outer scope of Cell 1\n",
    "    BASE_EXPERIMENT_NAME = baseline_config.get('EXPERIMENT_NAME', 'string_ca_subgraph_AIFM1_CORRECTED')\n",
    "    globals()['BASE_EXPERIMENT_NAME'] = BASE_EXPERIMENT_NAME\n",
    "    print(f\"  Base Experiment Name loaded and set globally: {BASE_EXPERIMENT_NAME}\")\n",
    "    # --- END MODIFIED ---\n",
    "\n",
    "\n",
    "    # --- Extract other relevant globals from baseline config ---\n",
    "    MASTER_SEED = baseline_config.get('MASTER_SEED', 42) # Keep seed for consistency if needed\n",
    "    TARGET_NODE_ID = baseline_config.get('TARGET_NODE_ID') # Target ID for baselines\n",
    "    TARGET_NODE_NAME = baseline_config.get('TARGET_NODE_NAME', 'TargetProtein') # Target Name\n",
    "\n",
    "    # Set these as globals for helper functions\n",
    "    globals()['MASTER_SEED'] = MASTER_SEED\n",
    "    globals()['TARGET_NODE_ID'] = TARGET_NODE_ID\n",
    "    globals()['TARGET_NODE_NAME'] = TARGET_NODE_NAME\n",
    "    globals()['OUTPUT_DIR_SIMULATIONS'] = OUTPUT_DIR_SIMULATIONS\n",
    "    globals()['ANALYSIS_DIR_BASE'] = ANALYSIS_DIR_BASE\n",
    "    # --- END Extract and Set GLOBALS ---\n",
    "\n",
    "    # --- MODIFIED: Extract and set all necessary API/Enrichment parameters as GLOBALS ---\n",
    "    # These are needed by the biological analysis helpers loaded later\n",
    "    SPECIES_ID = baseline_config.get('SPECIES_ID', \"9606\")\n",
    "    STRING_API_URL = baseline_config.get('STRING_API_URL', \"https://string-db.org/api\")\n",
    "    STRING_MAPPING_ENDPOINT = baseline_config.get('STRING_MAPPING_ENDPOINT', \"/tsv/get_string_ids\")\n",
    "    BATCH_SIZE = baseline_config.get('BATCH_SIZE', 100)\n",
    "    MAX_RETRIES = baseline_config.get('MAX_RETRIES', 5)\n",
    "    RETRY_DELAY = baseline_config.get('RETRY_DELAY', 7)\n",
    "    GO_ENRICHMENT_LIBRARY = baseline_config.get('GO_ENRICHMENT_LIBRARY', 'GO_Biological_Process_2023')\n",
    "    TOP_N_ENRICHMENT_TERMS = baseline_config.get('TOP_N_ENRICHMENT_TERMS', 15)\n",
    "\n",
    "    globals()['SPECIES_ID'] = SPECIES_ID\n",
    "    globals()['STRING_API_URL'] = STRING_API_URL\n",
    "    globals()['STRING_MAPPING_ENDPOINT'] = STRING_MAPPING_ENDPOINT\n",
    "    globals()['BATCH_SIZE'] = BATCH_SIZE\n",
    "    globals()['MAX_RETRIES'] = MAX_RETRIES\n",
    "    globals()['RETRY_DELAY'] = RETRY_DELAY\n",
    "    globals()['GO_ENRICHMENT_LIBRARY'] = GO_ENRICHMENT_LIBRARY\n",
    "    globals()['TOP_N_ENRICHMENT_TERMS'] = TOP_N_ENRICHMENT_TERMS\n",
    "    # --- END MODIFIED ---\n",
    "\n",
    "\n",
    "except FileNotFoundError as e: print(f\"❌ ERROR: {e}\"); config_load_error = True\n",
    "except Exception as e: print(f\"❌ Error loading config data: {e}\"); traceback.print_exc(limit=1); config_load_error = True\n",
    "\n",
    "\n",
    "if not config_load_error:\n",
    "    # --- Define Consistent Dynamic Region Analysis Parameters ---\n",
    "    # Based on user's clarified definition (time-averaged absolute change over window)\n",
    "\n",
    "    # 1. Window: Use the last 20% of steps\n",
    "    DYNAMIC_WINDOW_FRACTION = 0.20\n",
    "    # 2. Metric: Time-averaged absolute state CHANGE over the window (Act/Inh only)\n",
    "    DYNAMIC_METRIC_NAME = \"Time-Avg Abs Change (|Act_t+1-Act_t|, |Inh_t+1-Inh_t|)\" # Descriptive name\n",
    "    DYNAMIC_METRIC_KEY = 'time_avg_abs_change' # Internal key\n",
    "    # 3. Threshold: Above the 80th-percentile of this metric across all nodes in the final step\n",
    "    DYNAMIC_THRESHOLD_TYPE = 'percentile'\n",
    "    DYNAMIC_THRESHOLD_VALUE = 80 # 80th percentile\n",
    "\n",
    "    # --- Set these parameters as GLOBALS ---\n",
    "    globals()['DYNAMIC_WINDOW_FRACTION'] = DYNAMIC_WINDOW_FRACTION\n",
    "    globals()['DYNAMIC_METRIC_NAME'] = DYNAMIC_METRIC_NAME\n",
    "    globals()['DYNAMIC_METRIC_KEY'] = DYNAMIC_METRIC_KEY\n",
    "    globals()['DYNAMIC_THRESHOLD_TYPE'] = DYNAMIC_THRESHOLD_TYPE\n",
    "    globals()['DYNAMIC_THRESHOLD_VALUE'] = DYNAMIC_THRESHOLD_VALUE\n",
    "    # --- END Set GLOBALS ---\n",
    "\n",
    "    print(\"\\n--- Consistent Dynamic Region Analysis Parameters ---\")\n",
    "    print(f\"  Window: Last {DYNAMIC_WINDOW_FRACTION*100:.0f}% of steps\")\n",
    "    print(f\"  Metric: {DYNAMIC_METRIC_NAME}\")\n",
    "    print(f\"  Threshold: Above {DYNAMIC_THRESHOLD_VALUE}th percentile of metric values across nodes\")\n",
    "\n",
    "\n",
    "    # --- Define Output Directory for this Analysis Notebook ---\n",
    "    # Standardized global name to OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS\n",
    "    OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS = os.path.join(ANALYSIS_DIR_BASE, \"Dynamic_Biological_Analysis\")\n",
    "    os.makedirs(ANALYSIS_DIR_BASE, exist_ok=True) # Ensure base analysis dir exists\n",
    "    os.makedirs(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, exist_ok=True) # Ensure specific analysis dir exists\n",
    "    globals()['OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS'] = OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS # Set as global\n",
    "    print(f\"\\nAnalysis outputs will be saved in: {os.path.join(os.getcwd(), OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS)}\") # Show full path\n",
    "\n",
    "    # --- Define INPUT_DIR_DYNAMIC_REGIONS ---\n",
    "    # This is the output directory of ablation_09\n",
    "    INPUT_DIR_DYNAMIC_REGIONS = os.path.join(ANALYSIS_DIR_BASE, \"Dynamic_Analysis_Across_Runs\")\n",
    "    # --- FIXED: Set INPUT_DIR_DYNAMIC_REGIONS as a global ---\n",
    "    globals()['INPUT_DIR_DYNAMIC_REGIONS'] = INPUT_DIR_DYNAMIC_REGIONS\n",
    "    # --- END FIXED ---\n",
    "\n",
    "    # --- ADDED: Define OUTPUT_DIR_DYNAMIC_ANALYSIS as a global ---\n",
    "    # In ablation_09, the OUTPUT_DIR_DYNAMIC_ANALYSIS variable points to the directory where ablation_09 saves its own outputs.\n",
    "    # This is the same directory as INPUT_DIR_DYNAMIC_REGIONS for THIS notebook (ablation_10).\n",
    "    # So, set this global to the same path as INPUT_DIR_DYNAMIC_REGIONS.\n",
    "    globals()['OUTPUT_DIR_DYNAMIC_ANALYSIS'] = INPUT_DIR_DYNAMIC_REGIONS\n",
    "    # --- END ADDED ---\n",
    "\n",
    "\n",
    "    # --- Save Analysis Config ---\n",
    "    analysis_config_save_path = os.path.join(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, \"dynamic_biological_analysis_config.json\")\n",
    "    try:\n",
    "        analysis_config_dict = {\n",
    "             'INPUT_DIR_DYNAMIC_REGIONS': INPUT_DIR_DYNAMIC_REGIONS, # This path is used by Cell 4\n",
    "             'SPECIES_ID': SPECIES_ID, # Use the variable name (now global)\n",
    "             'STRING_API_URL': STRING_API_URL, # Use the variable name (now global)\n",
    "             'STRING_MAPPING_ENDPOINT': STRING_MAPPING_ENDPOINT, # Use the variable name (now global)\n",
    "             'BATCH_SIZE': BATCH_SIZE, # Use the variable name (now global)\n",
    "             'MAX_RETRIES': MAX_RETRIES, # Use the variable name (now global)\n",
    "             'RETRY_DELAY': RETRY_DELAY, # Use the variable name (now global)\n",
    "             'GO_ENRICHMENT_LIBRARY': GO_ENRICHMENT_LIBRARY, # Use the variable name (now global)\n",
    "             'TOP_N_ENRICHMENT_TERMS': TOP_N_ENRICHMENT_TERMS, # Use the variable name (now global)\n",
    "             'ANALYSIS_DIR_BASE': ANALYSIS_DIR_BASE,\n",
    "             'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS': OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS,\n",
    "             'TARGET_NODE_ID': TARGET_NODE_ID, # Use the variable name\n",
    "             'TARGET_NODE_NAME': TARGET_NODE_NAME, # Use the variable name\n",
    "             'MASTER_SEED': MASTER_SEED # Include seed for record-keeping\n",
    "        }\n",
    "        with open(analysis_config_save_path, 'w') as f:\n",
    "            json.dump(analysis_config_dict, f, indent=4, default=str) # Use default=str for numpy types\n",
    "        print(f\"   ✅ Saved analysis configuration to {analysis_config_save_path}\")\n",
    "    except Exception as e: print(f\"   ⚠️ Warning: Could not save analysis configuration: {e}\")\n",
    "\n",
    "else:\n",
    "     print(\"\\n❌ Configuration loading failed. Analysis cannot proceed.\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 1: Configuration loaded and analysis parameters defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a6f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 1.1: Defining Canonical Helper Functions for Dynamic Biological Analysis (2025-04-28 21:18:27) ---\n",
      "\n",
      " ✅ Cell 1.1: Canonical biological analysis helper functions defined (Using GLOBALS for config).\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.1: Canonical Helper Functions for Dynamic Biological Analysis\n",
    "# Description: Defines ALL helper functions required by subsequent cells in THIS notebook\n",
    "#              (ablation_10). Includes functions for loading histories, calculating\n",
    "#              dynamic region metrics, loading static baselines, mapping IDs, running\n",
    "#              enrichment, and plotting. These functions are self-contained within this notebook.\n",
    "#              MODIFIED: Includes function for calculating time-averaged absolute CHANGE.\n",
    "#              MODIFIED: Added warning suppression in load_history_dfs for non-numeric checks.\n",
    "#              MODIFIED: Corrected logic in load_static_baseline_nodes to handle file not found more explicitly\n",
    "#                        and avoid printing the \"Loaded X nodes for baseline check\" message if the file isn't used.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import pickle # Needed for loading pkl files\n",
    "import traceback\n",
    "import time # Needed for time.strftime if functions use it\n",
    "import requests # Needed for API calls in map_string_ids_to_gene_symbols_api\n",
    "import io # Needed for io.StringIO in API mapping\n",
    "from tqdm.notebook import tqdm # Use notebook version for progress bars\n",
    "\n",
    "# Ensure gseapy is available (checked in Cell 0 or earlier)\n",
    "try:\n",
    "    import gseapy as gp # Rename to gp for consistency\n",
    "except ImportError:\n",
    "    gp = None # Set to None to allow checks later\n",
    "\n",
    "\n",
    "print(f\"\\n--- Cell 1.1: Defining Canonical Helper Functions for Dynamic Biological Analysis ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "\n",
    "# === HELPER FUNCTIONS COPIED FROM ABLATION_09 CELL 1.1 (DYNAMIC ANALYSIS) ===\n",
    "\n",
    "# --- Helper Function: Load History DataFrames from Run Folder ---\n",
    "# COPIED from ablation_09 Cell 1.1\n",
    "def load_history_dfs(run_folder_name, base_sim_output_dir):\n",
    "    \"\"\"\n",
    "    Loads activation and inhibition history DataFrames from a simulation run folder.\n",
    "    Returns (act_df, inh_df) or (None, None) on failure.\n",
    "    Looks for filenames like 'activation_history_analysis.csv' or 'activation_history.csv'.\n",
    "    Suppresses UserWarnings during non-numeric dtype checks.\n",
    "    \"\"\"\n",
    "    act_df = None; inh_df = None\n",
    "    run_output_dir = os.path.join(base_sim_output_dir, run_folder_name)\n",
    "    # Prioritize 'analysis' suffixed files, fall back to standard names\n",
    "    act_paths = [os.path.join(run_output_dir, \"activation_history_analysis.csv\"), os.path.join(run_output_dir, \"activation_history.csv\")]\n",
    "    inh_paths = [os.path.join(run_output_dir, \"inhibition_history_analysis.csv\"), os.path.join(run_output_dir, \"inhibition_history.csv\")]\n",
    "\n",
    "    try:\n",
    "        found_act_path = next((p for p in act_paths if os.path.exists(p)), None)\n",
    "        if found_act_path:\n",
    "            # Use float dtype hint during read for performance/robustness if possible\n",
    "            act_df = pd.read_csv(found_act_path, index_col=0, dtype=float, low_memory=False)\n",
    "            # print(f\"    Loaded Act history from: {os.path.basename(found_act_path)}\") # Too verbose\n",
    "        else: warnings.warn(f\"    Act history not found for {run_folder_name}.\")\n",
    "\n",
    "        found_inh_path = next((p for p in inh_paths if os.path.exists(p)), None)\n",
    "        if found_inh_path:\n",
    "             inh_df = pd.read_csv(found_inh_path, index_col=0, dtype=float, low_memory=False)\n",
    "             # print(f\"    Loaded Inh history from: {os.path.basename(found_inh_path)}\") # Too verbose\n",
    "        else: warnings.warn(f\"    Inh history not found for {run_folder_name}.\")\n",
    "\n",
    "        if act_df is None or inh_df is None: return None, None # Return None if either failed to load\n",
    "        if act_df.empty or inh_df.empty: warnings.warn(f\"    Loaded history DF(s) empty for {run_folder_name}.\"); return None, None\n",
    "        if act_df.shape != inh_df.shape: warnings.warn(f\"    History DF shape mismatch for {run_folder_name}: Act={act_df.shape}, Inh={inh_df.shape}. Using common steps/nodes.\");\n",
    "        # Harmonize dataframes based on common indices and columns\n",
    "        common_indices = act_df.index.intersection(inh_df.index)\n",
    "        common_columns = act_df.columns.intersection(inh_df.columns)\n",
    "        act_df = act_df.loc[common_indices, common_columns]\n",
    "        inh_df = inh_df.loc[common_indices, common_columns]\n",
    "        if act_df.empty or inh_df.empty: warnings.warn(f\"    DFs became empty after harmonization for {run_folder_name}.\"); return None, None\n",
    "\n",
    "        # --- MODIFIED: Add warning suppression around the numeric dtype check ---\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\") # Suppress UserWarnings (like 'Non-numeric data detected')\n",
    "            # Check for potential non-numeric data after loading - this check can raise UserWarnings\n",
    "            # if there are mixed types or pandas isn't sure. The dtype=float hint helps.\n",
    "            if not pd.api.types.is_numeric_dtype(act_df.values) or not pd.api.types.is_numeric_dtype(inh_df.values):\n",
    "                # If despite dtype=float, it's still not purely numeric, try coercion.\n",
    "                 # The original warning message will be suppressed by the catch_warnings block.\n",
    "                 print(f\"    Attempting coercion to numeric for history DFs for {run_folder_name}.\")\n",
    "                 act_df = act_df.apply(pd.to_numeric, errors='coerce')\n",
    "                 inh_df = inh_df.apply(pd.to_numeric, errors='coerce')\n",
    "                 if act_df.isnull().all().all() or inh_df.isnull().all().all():\n",
    "                      warnings.warn(f\"    Coercion failed, DFs are all NaN for {run_folder_name}. Skipping.\")\n",
    "                      return None, None # Return None if coercion fails completely\n",
    "        # --- END MODIFIED ---\n",
    "\n",
    "\n",
    "        return act_df, inh_df # Return successfully loaded/harmonized DFs\n",
    "\n",
    "    except FileNotFoundError:\n",
    "         # This case is covered by the checks inside the try block\n",
    "         return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error loading history DataFrames for {run_folder_name}: {e}\")\n",
    "        traceback.print_exc(limit=1)\n",
    "        return None, None # Return None on any loading/processing error\n",
    "\n",
    "# --- Helper Function: Calculate Time-Averaged Absolute State Change Metric ---\n",
    "# COPIED from ablation_09 Cell 1.1\n",
    "# MODIFIED: Implements the user's specified metric calculation.\n",
    "# MODIFIED: Accesses parameters from GLOBALS as expected.\n",
    "def calculate_time_avg_abs_change_metric(act_history_df, inh_history_df):\n",
    "    \"\"\"\n",
    "    Calculates the time-averaged absolute change in Act/Inh for each node\n",
    "    over the final window, based on the user's defined metric.\n",
    "    Returns a pandas Series {node_id: metric_value} or None.\n",
    "    \"\"\"\n",
    "    # Access parameters from GLOBALS\n",
    "    window_fraction = globals().get('DYNAMIC_WINDOW_FRACTION', 0.20)\n",
    "    metric_key = globals().get('DYNAMIC_METRIC_KEY', 'time_avg_abs_change') # For internal verification\n",
    "\n",
    "    if act_history_df is None or inh_history_df is None or act_history_df.empty or inh_history_df.empty:\n",
    "        warnings.warn(f\"    Cannot calculate '{metric_key}': Input DataFrames are missing or empty.\")\n",
    "        return None\n",
    "\n",
    "    if not act_history_df.index.equals(inh_history_df.index) or not act_history_df.columns.equals(inh_history_df.columns):\n",
    "        warnings.warn(f\"    Act/Inh DataFrames indices/columns do not match, using intersection.\")\n",
    "        common_indices = act_history_df.index.intersection(inh_history_df.index)\n",
    "        common_columns = act_history_df.columns.intersection(inh_history_df.columns)\n",
    "        act_history_df = act_history_df.loc[common_indices, common_columns]\n",
    "        inh_history_df = inh_history_df.loc[common_indices, common_columns]\n",
    "        if act_history_df.empty:\n",
    "             warnings.warn(f\"    DFs became empty after harmonization, cannot calculate '{metric_key}'.\")\n",
    "             return pd.Series(np.nan, index=[]) # Return empty series\n",
    "    # Else DFs should be aligned\n",
    "\n",
    "\n",
    "    num_steps_hist = len(act_history_df)\n",
    "    if num_steps_hist < 2:\n",
    "        warnings.warn(f\"    History has < 2 steps ({num_steps_hist}), cannot calculate state change.\")\n",
    "        return pd.Series(np.nan, index=act_history_df.columns) # Return Series of NaN keyed by node ID\n",
    "\n",
    "\n",
    "    # Calculate step-wise absolute change for Act and Inh\n",
    "    # Use diff() which calculates change between adjacent steps\n",
    "    # The first row of diff() will be NaN\n",
    "    abs_change_act = act_history_df.diff().abs()\n",
    "    abs_change_inh = inh_history_df.diff().abs()\n",
    "\n",
    "    # Combine absolute changes (element-wise maximum)\n",
    "    # Handle potential dimension mismatches if harmonization failed strangely\n",
    "    if not abs_change_act.index.equals(abs_change_inh.index) or not abs_change_act.columns.equals(abs_change_inh.columns):\n",
    "         warnings.warn(\"    Absolute change DFs indices/columns mismatch after diff().\")\n",
    "         # Attempt re-harmonization based on common indices/columns again\n",
    "         common_indices = abs_change_act.index.intersection(abs_change_inh.index)\n",
    "         common_columns = abs_change_act.columns.intersection(abs_change_inh.columns)\n",
    "         abs_change_act = abs_change_act.loc[common_indices, common_columns]\n",
    "         abs_change_inh = abs_change_inh.loc[common_indices, common_columns]\n",
    "         if abs_change_act.empty:\n",
    "             warnings.warn(\"    Change DFs became empty after harmonization.\")\n",
    "             return pd.Series(np.nan, index=[])\n",
    "\n",
    "\n",
    "    abs_change_max = pd.DataFrame(np.maximum(abs_change_act.values, abs_change_inh.values), index=abs_change_act.index, columns=abs_change_act.columns)\n",
    "\n",
    "\n",
    "    # Determine the window start index\n",
    "    # Start window from the beginning of the history if it's shorter than window size\n",
    "    window_length = max(1, int(window_fraction * num_steps_hist)) # Ensure window is at least 1 step\n",
    "    # Window should be applied to the *changes*, which start from step 1 (index 1)\n",
    "    # So the number of change steps is num_steps_hist - 1\n",
    "    num_change_steps = num_steps_hist - 1\n",
    "    window_start_change_index = max(1, num_change_steps - window_length + 1) # Index in abs_change_max DataFrame\n",
    "\n",
    "    # Select the window from the changes DataFrame (iloc uses integer index)\n",
    "    window_changes = abs_change_max.iloc[window_start_change_index :]\n",
    "\n",
    "    if window_changes.empty:\n",
    "         warnings.warn(f\"    Calculated window is empty ({window_length} steps from {num_steps_hist} total). Cannot calculate time-averaged change.\")\n",
    "         # Return Series of NaN keyed by node ID, matching original DataFrame columns\n",
    "         return pd.Series(np.nan, index=act_history_df.columns)\n",
    "\n",
    "\n",
    "    # Calculate the time-averaged change over the window for each node (mean across time steps)\n",
    "    # Use .mean(axis=0) on the DataFrame for mean across rows (time) for each column (node)\n",
    "    # Handle potential NaNs within the window data if they exist\n",
    "    time_averaged_metric_per_node = window_changes.mean(axis=0) # Mean across rows (time)\n",
    "\n",
    "    # Return as a pandas Series indexed by node ID\n",
    "    return time_averaged_metric_per_node\n",
    "\n",
    "\n",
    "# --- Helper Function: Identify Dynamic Nodes based on Metric ---\n",
    "# COPIED from ablation_09 Cell 1.1\n",
    "# MODIFIED: Implements the user's specified thresholding logic.\n",
    "# MODIFIED: Accesses parameters from GLOBALS as expected.\n",
    "def identify_dynamic_nodes(node_metric_series):\n",
    "    \"\"\"\n",
    "    Identifies nodes whose metric value is above the percentile threshold.\n",
    "    Returns a list of node IDs.\n",
    "    \"\"\"\n",
    "    # Access parameters from GLOBALS\n",
    "    threshold_type = globals().get('DYNAMIC_THRESHOLD_TYPE', 'percentile')\n",
    "    threshold_value = globals().get('DYNAMIC_THRESHOLD_VALUE', 80)\n",
    "    metric_key = globals().get('DYNAMIC_METRIC_KEY', 'metric_value') # For warning messages\n",
    "\n",
    "    if node_metric_series is None or node_metric_series.empty:\n",
    "        warnings.warn(f\"    Cannot identify dynamic nodes: Input metric series is missing or empty.\")\n",
    "        return []\n",
    "\n",
    "    # Exclude NaN values from the metric distribution for threshold calculation\n",
    "    valid_metric_values = node_metric_series.dropna().values\n",
    "\n",
    "    if len(valid_metric_values) == 0:\n",
    "        warnings.warn(f\"    All metric values are NaN for '{node_metric_series.name if hasattr(node_metric_series,'name') else 'Series'}'. Cannot determine threshold or dynamic nodes.\")\n",
    "        return []\n",
    "    if len(valid_metric_values) < 2 and threshold_type == 'percentile':\n",
    "         warnings.warn(f\"    Fewer than 2 valid metric values ({len(valid_metric_values)}) for '{node_metric_series.name if hasattr(node_metric_series,'name') else 'Series'}', percentile threshold unreliable/impossible. Using median as threshold fallback.\")\n",
    "         # Fallback to median if percentile is requested but insufficient data\n",
    "         threshold_type = 'absolute'\n",
    "         threshold_value = np.median(valid_metric_values) if valid_metric_values.size > 0 else 0.0\n",
    "\n",
    "\n",
    "    actual_threshold = np.nan # Initialize\n",
    "\n",
    "    if threshold_type == 'percentile':\n",
    "        percentile = threshold_value # Use the value directly (e.g., 80)\n",
    "        if not (0 <= percentile <= 100):\n",
    "             warnings.warn(f\"    Percentile threshold value out of bounds (0-100): {percentile}. Using 80th percentile.\")\n",
    "             percentile = 80\n",
    "        try:\n",
    "            actual_threshold = np.percentile(valid_metric_values, percentile)\n",
    "            # print(f\"    Using {percentile}th percentile threshold: {actual_threshold:.6f}\") # Print in calling cell\n",
    "        except Exception as e:\n",
    "             warnings.warn(f\"    Error calculating percentile threshold: {e}. Cannot identify dynamic nodes.\")\n",
    "             return [] # Return empty list on error\n",
    "\n",
    "    elif threshold_type == 'absolute':\n",
    "        actual_threshold = threshold_value # Use the value directly\n",
    "        # print(f\"    Using absolute threshold: {actual_threshold:.6f}\") # Print in calling cell\n",
    "    else:\n",
    "        warnings.warn(f\"    Unknown threshold type: '{threshold_type}'. Cannot identify dynamic nodes.\")\n",
    "        return [] # Return empty list on unknown type\n",
    "\n",
    "    if pd.isna(actual_threshold): # Should be set by now, but check for safety\n",
    "         warnings.warn(\"    Actual threshold value is NaN. Cannot identify dynamic nodes.\")\n",
    "         return []\n",
    "\n",
    "    # Identify nodes whose metric value is >= the calculated/determined threshold\n",
    "    # Use .loc to apply threshold to the Series and get the index (node IDs)\n",
    "    # Handle potential NaNs in the original series by filling temporarily for comparison >= threshold\n",
    "    dynamic_nodes_series = node_metric_series[np.nan_to_num(node_metric_series, nan=-np.inf) >= actual_threshold]\n",
    "\n",
    "    # Return the list of node IDs\n",
    "    return dynamic_nodes_series.index.tolist()\n",
    "\n",
    "\n",
    "# === HELPER FUNCTIONS COPIED FROM ABLATION_08 CELL 2 (BIOLOGICAL ANALYSIS) ===\n",
    "\n",
    "# --- Helper Function: Load STRING IDs from File (Duplicate, exists above, keep only one) ---\n",
    "# The definition is identical to the one above in this combined block. Keep only one.\n",
    "# def load_string_ids_from_txt(filename):\n",
    "#     \"\"\"Loads STRING IDs directly from a text file (one ID per line).\"\"\"\n",
    "#     # ... (identical code) ...\n",
    "#     pass # Definition is above\n",
    "\n",
    "\n",
    "# --- Helper Function: Map STRING IDs to Gene Symbols via API with Caching ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "# MODIFIED: Use GLOBALS for config parameters (SPECIES_ID, API_URL, etc.)\n",
    "# MODIFIED: Fixed SyntaxError in map_string_ids_to_gene_symbols_api.\n",
    "# MODIFIED: Fixed NameError in map_string_ids_to_gene_symbols_api by using the correct variable name.\n",
    "# MODIFIED: Corrected output dir variable name to OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS.\n",
    "def map_string_ids_to_gene_symbols_api(string_ids, description=\"IDs\"):\n",
    "    \"\"\"Maps STRING IDs (e.g., 9606.ENSP...) to Gene Symbols using the STRING API with caching.\"\"\"\n",
    "    # Access globals for configuration\n",
    "    species_id = globals().get('SPECIES_ID', '9606')\n",
    "    api_url = globals().get('STRING_API_URL', 'https://string-db.org/api')\n",
    "    mapping_endpoint = globals().get('STRING_MAPPING_ENDPOINT', '/tsv/get_string_ids')\n",
    "    batch_size = globals().get('BATCH_SIZE', 100)\n",
    "    max_retries = globals().get('MAX_RETRIES', 3)\n",
    "    retry_delay = globals().get('RETRY_DELAY', 5)\n",
    "    # MODIFIED: Use the correct global variable for this notebook's output directory\n",
    "    output_dir_analysis_base = globals().get('OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS', '.')\n",
    "    # END MODIFIED\n",
    "\n",
    "    if not string_ids: print(f\"⚠️ No input STRING IDs provided for {description}. Skipping mapping.\"); return []\n",
    "    print(f\"Mapping {len(string_ids)} {description} IDs...\")\n",
    "\n",
    "    # --- Caching Logic ---\n",
    "    # Use a consistent cache file name within this notebook's output directory\n",
    "    mapping_cache_file = os.path.join(output_dir_analysis_base, 'string_id_to_gene_symbol_map_cache.pkl')\n",
    "    cached_map = {}\n",
    "    if os.path.exists(mapping_cache_file):\n",
    "        try:\n",
    "             with open(mapping_cache_file, 'rb') as f_cache: cached_map = pickle.load(f_cache)\n",
    "             if not isinstance(cached_map, dict): cached_map = {}\n",
    "             else: print(f\"  ℹ️ Loaded {len(cached_map)} mappings from cache: {mapping_cache_file}\")\n",
    "        except Exception as e_cache: print(f\"  ⚠️ Warning: Could not load mapping cache: {e_cache}. Will rebuild.\")\n",
    "\n",
    "    # --- Identify IDs needing API call ---\n",
    "    ids_needing_map = sorted(list(set(string_ids) - set(cached_map.keys())))\n",
    "    mapped_symbols_dict = cached_map.copy() # Start with cache. THIS IS THE CORRECT VARIABLE NAME.\n",
    "\n",
    "    if identifiers_to_map:\n",
    "        print(f\"  🌐 Mapping {len(identifiers_to_map)} IDs via STRING API...\")\n",
    "        identifiers_to_map_api = [sid for sid in ids_needing_map if isinstance(sid, str) and sid.startswith(str(species_id) + '.')]\n",
    "        if not identifiers_to_map_api: print(\"    No valid STRING IDs (starting with species ID) found for mapping via API.\");\n",
    "        else:\n",
    "            api_map_url = f\"{api_url}{mapping_endpoint}\"; num_batches = (len(identifiers_to_map_api) + batch_size - 1)//batch_size;\n",
    "            use_tqdm = num_batches > 1;\n",
    "            batch_iterator = tqdm(range(num_batches), desc=f\"API Mapping ({description})\", leave=False) if use_tqdm else range(num_batches);\n",
    "            api_errors = 0;\n",
    "            for i in batch_iterator:\n",
    "                 batch = identifiers_to_map_api[i*batch_size:(i+1)*batch_size];\n",
    "                 payload = {'identifiers': \"\\r\".join(batch), 'species': species_id, 'limit': 1, 'echo_query': 1, 'caller_identity': 'NetworkAutomatonSim/1.0'};\n",
    "                 retries = 0; success = False; response = None;\n",
    "                 while retries < max_retries and not success:\n",
    "                      try:\n",
    "                           response = requests.post(api_map_url, data=payload, timeout=60);\n",
    "                           response.raise_for_status();\n",
    "                           content_type = response.headers.get('Content-Type','');\n",
    "                           if 'text/tab-separated-values' in content_type:\n",
    "                                text = response.text;\n",
    "                                if text and text.strip():\n",
    "                                     mapping_df = pd.read_csv(io.StringIO(text), sep='\\t')\n",
    "                                     if 'preferredName' in mapping_df.columns and 'queryItem' in mapping_df.columns:\n",
    "                                          for qid, name in zip(mapping_df['queryItem'], mapping_df['preferredName']):\n",
    "                                               if pd.notna(name) and name: mapped_symbols_dict[qid] = str(name)\n",
    "                                     else: warnings.warn(f\"API response missing expected columns ('preferredName', 'queryItem') in batch {i+1}.\"); api_errors += 1; break\n",
    "                                success = True;\n",
    "                           else: warnings.warn(f\"Unexpected API format batch {i+1}: {content_type}. Retrying...\"); retries += 1; time.sleep(retry_delay*(1+retries));\n",
    "                      except requests.exceptions.Timeout:\n",
    "                           retries += 1;\n",
    "                           print(f\"\\n   Timeout on API call (Batch {i+1}, Retry {retries}/{max_retries}). Sleeping...\")\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      except requests.exceptions.RequestException as e:\n",
    "                           retries += 1;\n",
    "                           print(f\"   API Request Error (Batch {i+1}, Retry {retries}/{max_retries}): {e}. Sleeping...\")\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      except pd.errors.EmptyDataError:\n",
    "                           success = True;\n",
    "                      except Exception as e:\n",
    "                           retries += 1;\n",
    "                           print(f\"   Unexpected Error Processing API Response (Batch {i+1}, Retry {retries}/{max_retries}): {e}\")\n",
    "                           traceback.print_exc(limit=1)\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      if retries >= max_retries and not success:\n",
    "                           print(f\"   ❌ API call failed for Batch {i+1} after {retries} retries.\")\n",
    "                           api_errors += 1\n",
    "            if api_errors > 0: warnings.warn(f\"🚨 API mapping failed for {api_errors}/{num_batches} batches after retries.\")\n",
    "            try:\n",
    "                with open(mapping_cache_file, 'wb') as f_cache: pickle.dump(mapped_symbols_dict, f_cache) # Use mapped_symbols_dict\n",
    "                print(f\"  ✅ Updated mapping cache saved ({len(mapped_symbols_dict)} total mappings).\") # Use mapped_symbols_dict\n",
    "            except Exception as e_save_cache: print(f\"  ⚠️ Error saving updated mapping cache: {e_save_cache}\")\n",
    "    else: print(\"  All required IDs found in cache.\");\n",
    "\n",
    "    mapped_symbols_list = []\n",
    "    for sid in string_ids:\n",
    "        mapped_val = mapped_symbols_dict.get(sid, sid) # Use mapped_symbols_dict\n",
    "        if isinstance(mapped_val, str) and not ('.' in mapped_val and mapped_val.split('.')[0] == str(species_id)):\n",
    "             mapped_symbols_list.append(mapped_val)\n",
    "    # print(f\"\\n✅ Mapping complete. Found {len(unique_gene_symbols)} unique Gene Symbols for {description}.\") # Printed in calling loop\n",
    "    return sorted(list(set(mapped_symbols_list))) # Return sorted unique list\n",
    "\n",
    "\n",
    "# --- Helper Function: Run Enrichment Analysis ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "# MODIFIED: Use GLOBALS for gene_sets and top_n defaults.\n",
    "def run_enrichment_simple(gene_list, description):\n",
    "     \"\"\"Runs gseapy enrichment analysis.\"\"\"\n",
    "     # Access globals for configuration\n",
    "     gene_sets = globals().get('GO_ENRICHMENT_LIBRARY', 'GO_Biological_Process_2023')\n",
    "     top_n = globals().get('TOP_N_ENRICHMENT_TERMS', 15)\n",
    "\n",
    "     if gp is None: print(\"❌ Cannot run enrichment: gseapy not available.\"); return None\n",
    "     if not isinstance(gene_list, list) or not all(isinstance(g, str) for g in gene_list):\n",
    "         print(f\"⚠️ Skip enrichment: Input gene_list is not a list of strings for '{description}'.\")\n",
    "         return None\n",
    "     cleaned_gene_list = [g for g in gene_list if g]\n",
    "     if not cleaned_gene_list or len(cleaned_gene_list) < 3:\n",
    "         # print(f\"⚠️ Skip enrichment '{description}': Needs >= 3 non-empty genes (found {len(cleaned_gene_list)}).\"); # Printed in calling loop\n",
    "         return pd.DataFrame()\n",
    "\n",
    "     print(f\"--- 📊 Performing Enrichment: {description} ({len(cleaned_gene_list)} genes) using '{gene_sets}' ---\")\n",
    "     try:\n",
    "          libs = gp.get_library_name(organism='Human')\n",
    "          valid_libs = [gene_sets] if gene_sets in libs else [] # Only use the specified library if valid\n",
    "          if not valid_libs:\n",
    "              print(f\"❌ Invalid or unavailable gene_set: '{gene_sets}'. Available examples: {libs[:5]}\")\n",
    "              return pd.DataFrame()\n",
    "\n",
    "          enr = gp.enrichr(gene_list=cleaned_gene_list, gene_sets=valid_libs, organism='Human', outdir=None, cutoff=0.05) # Use gp alias\n",
    "\n",
    "          if enr is None or not hasattr(enr, 'results') or enr.results.empty:\n",
    "               # print(f\"ℹ️ No significant terms found for '{description}' using library: '{gene_sets}'.\"); # Printed in calling loop\n",
    "               return pd.DataFrame()\n",
    "\n",
    "          results_df = enr.results.sort_values('Adjusted P-value').reset_index(drop=True)\n",
    "          # print(f\"🧬 Top {min(top_n, len(results_df))} Enriched Terms ('{gene_sets}'):\") # Printed in calling loop\n",
    "          # Format for display - only if needed, not for return value\n",
    "          # display_df=results_df[['Term', 'Adjusted P-value', 'Overlap', 'Genes']].head(top_n).copy()\n",
    "          # display_df['Adjusted P-value'] = display_df['Adjusted P-value'].map('{:.2e}'.format)\n",
    "          # display_df['Genes'] = display_df['Genes'].fillna('').astype(str).str.split(';').str[:5].apply(lambda x: ';'.join(x)+('...' if len(x)==5 else ''))\n",
    "          # print(display_df.to_string(index=False)); print(\"-\" * 70);\n",
    "\n",
    "          return results_df # Return the full DataFrame\n",
    "     except requests.exceptions.RequestException as e:\n",
    "          print(f\"❌ GSEAPY Enrichr Connection Error for '{description}': {e}\");\n",
    "          return None # Indicate error\n",
    "     except Exception as e:\n",
    "          print(f\"❌ Unexpected enrichment error for '{description}': {e}\");\n",
    "          traceback.print_exc(limit=2);\n",
    "          return None # Indicate error\n",
    "\n",
    "\n",
    "# --- Helper Function: Plot Enrichment Results ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "# MODIFIED: Use GLOBALS for top_n and output_dir defaults.\n",
    "# MODIFIED: Removed automatic saving and displaying; calling cell will handle that.\n",
    "def create_enrichment_plot_simple(results_df, title):\n",
    "    \"\"\"\n",
    "    Generates a horizontal bar plot for enrichment results.\n",
    "    Returns the matplotlib Figure object or None. Does NOT save or display automatically.\n",
    "    \"\"\"\n",
    "    # Access globals for configuration\n",
    "    top_n = globals().get('TOP_N_ENRICHMENT_TERMS', 15)\n",
    "\n",
    "    if not isinstance(results_df, pd.DataFrame) or results_df.empty: print(f\"⚠️ Plot function skipped for '{title}': No data.\"); return None\n",
    "    pval_col = 'Adjusted P-value'; term_col = 'Term'\n",
    "    if pval_col not in results_df.columns or term_col not in results_df.columns: print(f\"❌ Plot Error for '{title}': Missing columns '{pval_col}' or '{term_col}'.\"); return None\n",
    "\n",
    "    print(f\"--- Generating Enrichment Plot: {title} ---\")\n",
    "    plot_data = results_df.dropna(subset=[pval_col]).sort_values(by=pval_col, ascending=True).head(top_n).copy();\n",
    "    if plot_data.empty: print(\"ℹ️ No terms left to plot after filtering/sorting.\"); return None\n",
    "    epsilon = np.finfo(float).tiny\n",
    "    plot_data['-log10 Adj P'] = -np.log10(plot_data[pval_col].replace(0, epsilon) + epsilon)\n",
    "    plot_data.sort_values('-log10 Adj P', ascending=True, inplace=True)\n",
    "\n",
    "    num_terms_plotted = len(plot_data); fig_height = max(6, num_terms_plotted * 0.5)\n",
    "    fig, ax = plt.subplots(figsize=(10, fig_height))\n",
    "    sns.barplot(x='-log10 Adj P', y=term_col, data=plot_data, palette='viridis_r', ax=ax, orient='h')\n",
    "    ax.set_title(title, fontsize=13); ax.set_xlabel(f'-log10 ({pval_col})', fontsize=11); ax.set_ylabel('');\n",
    "    ax.tick_params(axis='y', labelsize=10); ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.xaxis.grid(True, linestyle='--', alpha=0.6); ax.yaxis.grid(False)\n",
    "    xlim_max = ax.get_xlim()[1]; text_offset = xlim_max * 0.01;\n",
    "    for i, bar_val in enumerate(plot_data['-log10 Adj P']): ax.text(bar_val + text_offset , i, f'{bar_val:.2f}', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Return the figure handle\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"\\n ✅ Cell 1.1: Canonical biological analysis helper functions defined (Using GLOBALS for config).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d47f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 2: Defining Canonical Helper Functions for Biological Analysis (2025-04-28 21:18:27) ---\n",
      "\n",
      " ✅ Cell 2: Canonical biological analysis helper functions defined (Using GLOBALS for config).\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Canonical Helper Functions for Biological Analysis\n",
    "# Description: Defines ALL necessary helper functions for loading IDs, mapping to\n",
    "#              gene symbols via STRING API, running GSEAPY enrichment, and plotting\n",
    "#              enrichment results. These functions are self-contained within this notebook.\n",
    "#              MODIFIED: Ensure functions use GLOBALS for config parameters.\n",
    "#              COPIED from ablation_08 Cell 2.\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import io\n",
    "from tqdm.notebook import tqdm # Use notebook version for progress bars\n",
    "import os\n",
    "import warnings\n",
    "import pickle # Needed for loading pkl files\n",
    "import traceback\n",
    "import numpy as np # For np.nan\n",
    "\n",
    "# Ensure gseapy is available (checked in Cell 0 or earlier)\n",
    "try:\n",
    "    import gseapy as gp # Rename to gp for consistency\n",
    "except ImportError:\n",
    "    gp = None # Set to None to allow checks later\n",
    "\n",
    "\n",
    "print(f\"\\n--- Cell 2: Defining Canonical Helper Functions for Biological Analysis ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "\n",
    "# --- Helper Function: Load STRING IDs from File ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "def load_string_ids_from_txt(filename):\n",
    "    \"\"\"Loads STRING IDs directly from a text file (one ID per line).\"\"\"\n",
    "    string_ids = []\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            string_ids = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"  📄 Loaded {len(string_ids)} STRING IDs from '{os.path.basename(filename)}'\")\n",
    "        if not string_ids: warnings.warn(f\"Loaded list from '{filename}' is empty.\")\n",
    "        return string_ids\n",
    "    except FileNotFoundError: print(f\"  ❌ Error: File not found at {filename}\"); return []\n",
    "    except Exception as e: print(f\"  ❌ Error loading data from {filename}: {e}\"); return []\n",
    "\n",
    "\n",
    "# --- Helper Function: Map STRING IDs to Gene Symbols via API with Caching ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "def map_string_ids_to_gene_symbols_api(string_ids, description=\"IDs\"):\n",
    "    \"\"\"Maps STRING IDs (e.g., 9606.ENSP...) to Gene Symbols using the STRING API with caching.\"\"\"\n",
    "    # Access globals for configuration\n",
    "    species_id = globals().get('SPECIES_ID', '9606')\n",
    "    api_url = globals().get('STRING_API_URL', 'https://string-db.org/api')\n",
    "    mapping_endpoint = globals().get('STRING_MAPPING_ENDPOINT', '/tsv/get_string_ids')\n",
    "    batch_size = globals().get('BATCH_SIZE', 100)\n",
    "    max_retries = globals().get('MAX_RETRIES', 3)\n",
    "    retry_delay = globals().get('RETRY_DELAY', 5)\n",
    "    output_dir_analysis_base = globals().get('OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS', '.') # Use this notebook's output dir\n",
    "\n",
    "    if not string_ids: print(f\"⚠️ No input STRING IDs provided for {description}. Skipping mapping.\"); return []\n",
    "    print(f\"Mapping {len(string_ids)} {description} IDs...\")\n",
    "\n",
    "    # --- Caching Logic ---\n",
    "    # Use a consistent cache file name within this notebook's output directory\n",
    "    mapping_cache_file = os.path.join(output_dir_analysis_base, 'string_id_to_gene_symbol_map_cache.pkl')\n",
    "    cached_map = {}\n",
    "    if os.path.exists(mapping_cache_file):\n",
    "        try:\n",
    "             with open(mapping_cache_file, 'rb') as f_cache: cached_map = pickle.load(f_cache)\n",
    "             if not isinstance(cached_map, dict): cached_map = {}\n",
    "             else: print(f\"  ℹ️ Loaded {len(cached_map)} mappings from cache: {mapping_cache_file}\")\n",
    "        except Exception as e_cache: print(f\"  ⚠️ Warning: Could not load mapping cache: {e_cache}. Will rebuild.\")\n",
    "\n",
    "    # --- Identify IDs needing API call ---\n",
    "    ids_needing_map = sorted(list(set(string_ids) - set(cached_map.keys())))\n",
    "    mapped_symbols_dict = cached_map.copy()\n",
    "\n",
    "    if ids_needing_map:\n",
    "        print(f\"  🌐 Mapping {len(ids_needing_map)} IDs via STRING API...\")\n",
    "        identifiers_to_map = [sid for sid in ids_needing_map if isinstance(sid, str) and sid.startswith(str(species_id) + '.')]\n",
    "        if not identifiers_to_map: print(\"    No valid STRING IDs (starting with species ID) found for mapping via API.\");\n",
    "        else:\n",
    "            api_map_url = f\"{api_url}{mapping_endpoint}\"; num_batches = (len(identifiers_to_map) + batch_size - 1)//batch_size;\n",
    "            use_tqdm = num_batches > 1;\n",
    "            batch_iterator = tqdm(range(num_batches), desc=f\"API Mapping ({description})\", leave=False) if use_tqdm else range(num_batches);\n",
    "            api_errors = 0;\n",
    "            for i in batch_iterator:\n",
    "                 batch = identifiers_to_map[i*batch_size:(i+1)*batch_size];\n",
    "                 payload = {'identifiers': \"\\r\".join(batch), 'species': species_id, 'limit': 1, 'echo_query': 1, 'caller_identity': 'NetworkAutomatonSim/1.0'};\n",
    "                 retries = 0; success = False; response = None;\n",
    "                 while retries < max_retries and not success:\n",
    "                      try:\n",
    "                           response = requests.post(api_map_url, data=payload, timeout=60);\n",
    "                           response.raise_for_status();\n",
    "                           content_type = response.headers.get('Content-Type','');\n",
    "                           if 'text/tab-separated-values' in content_type:\n",
    "                                text = response.text;\n",
    "                                if text and text.strip():\n",
    "                                     mapping_df = pd.read_csv(io.StringIO(text), sep='\\t')\n",
    "                                     if 'preferredName' in mapping_df.columns and 'queryItem' in mapping_df.columns:\n",
    "                                          for qid, name in zip(mapping_df['queryItem'], mapping_df['preferredName']):\n",
    "                                               if pd.notna(name) and name: mapped_symbols_dict[qid] = str(name)\n",
    "                                     else: warnings.warn(f\"API response missing expected columns ('preferredName', 'queryItem') in batch {i+1}.\"); api_errors += 1; break\n",
    "                                success = True;\n",
    "                           else: warnings.warn(f\"Unexpected API format batch {i+1}: {content_type}. Retrying...\"); retries += 1; time.sleep(retry_delay*(1+retries));\n",
    "                      except requests.exceptions.Timeout:\n",
    "                           retries += 1;\n",
    "                           print(f\"\\n   Timeout on API call (Batch {i+1}, Retry {retries}/{max_retries}). Sleeping...\")\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      except requests.exceptions.RequestException as e:\n",
    "                           retries += 1;\n",
    "                           print(f\"   API Request Error (Batch {i+1}, Retry {retries}/{max_retries}): {e}. Sleeping...\")\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      except pd.errors.EmptyDataError:\n",
    "                           success = True;\n",
    "                      except Exception as e:\n",
    "                           retries += 1;\n",
    "                           print(f\"   Unexpected Error Processing API Response (Batch {i+1}, Retry {retries}/{max_retries}): {e}\")\n",
    "                           traceback.print_exc(limit=1)\n",
    "                           time.sleep(retry_delay * (1 + retries));\n",
    "                      if retries >= max_retries and not success:\n",
    "                           print(f\"   ❌ API call failed for Batch {i+1} after {retries} retries.\")\n",
    "                           api_errors += 1\n",
    "            if api_errors > 0: warnings.warn(f\"🚨 API mapping failed for {api_errors}/{num_batches} batches after retries.\")\n",
    "            try:\n",
    "                with open(mapping_cache_file, 'wb') as f_cache: pickle.dump(mapped_symbols_dict, f_cache)\n",
    "                print(f\"  ✅ Updated mapping cache saved ({len(mapped_symbols_dict)} total mappings).\")\n",
    "            except Exception as e_save_cache: print(f\"  ⚠️ Error saving updated mapping cache: {e_save_cache}\")\n",
    "    else: print(\"  All required IDs found in cache.\");\n",
    "\n",
    "    mapped_symbols_list = []\n",
    "    for sid in string_ids:\n",
    "        mapped_val = mapped_symbols_dict.get(sid, sid)\n",
    "        if isinstance(mapped_val, str) and not ('.' in mapped_val and mapped_val.split('.')[0] == str(species_id)):\n",
    "             mapped_symbols_list.append(mapped_val)\n",
    "    unique_gene_symbols = sorted(list(set(mapped_symbols_list)))\n",
    "    # print(f\"\\n✅ Mapping complete. Found {len(unique_gene_symbols)} unique Gene Symbols for {description}.\") # Printed in calling loop\n",
    "    return unique_gene_symbols\n",
    "\n",
    "# --- Helper Function: Run Enrichment Analysis ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "# MODIFIED: Ensure functions use GLOBALS for gene_sets and top_n defaults.\n",
    "def run_enrichment_simple(gene_list, description):\n",
    "     \"\"\"Runs gseapy enrichment analysis.\"\"\"\n",
    "     # Access globals for configuration\n",
    "     gene_sets = globals().get('GO_ENRICHMENT_LIBRARY', 'GO_Biological_Process_2023')\n",
    "     top_n = globals().get('TOP_N_ENRICHMENT_TERMS', 15)\n",
    "\n",
    "     if gp is None: print(\"❌ Cannot run enrichment: gseapy not available.\"); return None\n",
    "     if not isinstance(gene_list, list) or not all(isinstance(g, str) for g in gene_list):\n",
    "         print(f\"⚠️ Skip enrichment: Input gene_list is not a list of strings for '{description}'.\")\n",
    "         return None\n",
    "     cleaned_gene_list = [g for g in gene_list if g]\n",
    "     if not cleaned_gene_list or len(cleaned_gene_list) < 3:\n",
    "         # print(f\"⚠️ Skip enrichment '{description}': Needs >= 3 non-empty genes (found {len(cleaned_gene_list)}).\"); # Printed in calling loop\n",
    "         return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "     print(f\"--- 📊 Performing Enrichment: {description} ({len(cleaned_gene_list)} genes) using '{gene_sets}' ---\")\n",
    "     try:\n",
    "          libs = gp.get_library_name(organism='Human')\n",
    "          valid_libs = [gene_sets] if gene_sets in libs else [] # Only use the specified library if valid\n",
    "          if not valid_libs:\n",
    "              print(f\"❌ Invalid or unavailable gene_set: '{gene_sets}'. Available examples: {libs[:5]}\")\n",
    "              return pd.DataFrame()\n",
    "\n",
    "          enr = gp.enrichr(gene_list=cleaned_gene_list, gene_sets=valid_libs, organism='Human', outdir=None, cutoff=0.05) # Use gp alias\n",
    "\n",
    "          if enr is None or not hasattr(enr, 'results') or enr.results.empty:\n",
    "               # print(f\"ℹ️ No significant terms found for '{description}' using library: '{gene_sets}'.\"); # Printed in calling loop\n",
    "               return pd.DataFrame() # Return empty DataFrame for consistency\n",
    "\n",
    "          # Process results\n",
    "          results_df = enr.results.sort_values('Adjusted P-value').reset_index(drop=True)\n",
    "          # print(f\"🧬 Top {min(top_n, len(results_df))} Enriched Terms ('{gene_sets}'):\") # Printed in calling loop\n",
    "          # Format for display - only if needed, not for return value\n",
    "          # display_df=results_df[['Term', 'Adjusted P-value', 'Overlap', 'Genes']].head(top_n).copy()\n",
    "          # display_df['Adjusted P-value'] = display_df['Adjusted P-value'].map('{:.2e}'.format)\n",
    "          # display_df['Genes'] = display_df['Genes'].fillna('').astype(str).str.split(';').str[:5].apply(lambda x: ';'.join(x)+('...' if len(x)==5 else ''))\n",
    "          # print(display_df.to_string(index=False)); print(\"-\" * 70);\n",
    "\n",
    "          return results_df # Return the full DataFrame\n",
    "     except requests.exceptions.RequestException as e:\n",
    "          print(f\"❌ GSEAPY Enrichr Connection Error for '{description}': {e}\");\n",
    "          return None # Indicate error\n",
    "     except Exception as e:\n",
    "          print(f\"❌ Unexpected enrichment error for '{description}': {e}\");\n",
    "          traceback.print_exc(limit=2);\n",
    "          return None # Indicate error\n",
    "\n",
    "\n",
    "# --- Helper Function: Plot Enrichment Results ---\n",
    "# COPIED from ablation_08 Cell 2\n",
    "# MODIFIED: Use GLOBALS for top_n and output_dir defaults.\n",
    "# MODIFIED: Removed automatic saving and displaying; calling cell will handle that.\n",
    "def create_enrichment_plot_simple(results_df, title):\n",
    "    \"\"\"\n",
    "    Generates a horizontal bar plot for enrichment results.\n",
    "    Returns the matplotlib Figure object or None. Does NOT save or display automatically.\n",
    "    \"\"\"\n",
    "    # Access globals for configuration\n",
    "    top_n = globals().get('TOP_N_ENRICHMENT_TERMS', 15)\n",
    "\n",
    "    if not isinstance(results_df, pd.DataFrame) or results_df.empty: print(f\"⚠️ Plot function skipped for '{title}': No data.\"); return None\n",
    "    pval_col = 'Adjusted P-value'; term_col = 'Term'\n",
    "    if pval_col not in results_df.columns or term_col not in results_df.columns: print(f\"❌ Plot Error for '{title}': Missing columns '{pval_col}' or '{term_col}'.\"); return None\n",
    "\n",
    "    print(f\"--- Generating Enrichment Plot: {title} ---\")\n",
    "    plot_data = results_df.dropna(subset=[pval_col]).sort_values(by=pval_col, ascending=True).head(top_n).copy()\n",
    "    if plot_data.empty: print(\"ℹ️ No terms left to plot after filtering/sorting.\"); return None\n",
    "    epsilon = np.finfo(float).tiny\n",
    "    plot_data['-log10 Adj P'] = -np.log10(plot_data[pval_col].replace(0, epsilon) + epsilon)\n",
    "    plot_data.sort_values('-log10 Adj P', ascending=True, inplace=True)\n",
    "\n",
    "    num_terms_plotted = len(plot_data); fig_height = max(6, num_terms_plotted * 0.5)\n",
    "    fig, ax = plt.subplots(figsize=(10, fig_height))\n",
    "    sns.barplot(x='-log10 Adj P', y=term_col, data=plot_data, palette='viridis_r', ax=ax, orient='h')\n",
    "    ax.set_title(title, fontsize=13); ax.set_xlabel(f'-log10 ({pval_col})', fontsize=11); ax.set_ylabel('');\n",
    "    ax.tick_params(axis='y', labelsize=10); ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.xaxis.grid(True, linestyle='--', alpha=0.6); ax.yaxis.grid(False)\n",
    "    xlim_max = ax.get_xlim()[1]; text_offset = xlim_max * 0.01;\n",
    "    for i, bar_val in enumerate(plot_data['-log10 Adj P']): ax.text(bar_val + text_offset , i, f'{bar_val:.2f}', va='center', ha='left', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Return the figure handle\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"\\n ✅ Cell 2: Canonical biological analysis helper functions defined (Using GLOBALS for config).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b0347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 3: Define Run Folders (2025-04-28 21:18:27) ---\n",
      "Defined mapping of run labels to expected folder names for dynamic analysis:\n",
      "  'H+P (2D Ref)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonicPheromone_REF'\n",
      "  'P-Only (2D)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearPheromoneOnly'\n",
      "  'H-Only (2D)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonicOnly'\n",
      "  'H+3D-PH (Coupled)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim3D'\n",
      "  'H+5D-PH (Coupled)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim5D'\n",
      "  'H+5D-PH (Decoupled)': 'string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim5D_DecoupledDiff'\n",
      "  'H+4D-Bio (AIFM1)': 'string_ca_subgraph_AIFM1_CORRECTED_Harmonic4DBio'\n",
      "\n",
      " ✅ Cell 3: Run folders defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define Run Folders\n",
    "# Description: Defines the mapping from human-readable labels to the actual\n",
    "#              experiment folder names for ALL relevant runs (01-07).\n",
    "#              MODIFIED: Sets run_folder_map as a global variable.\n",
    "#              COPIED from ablation_09 Cell 2.\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(f\"\\n--- Cell 3: Define Run Folders ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# Ensure BASE_EXPERIMENT_NAME is defined globally (should be from Cell 1)\n",
    "if 'BASE_EXPERIMENT_NAME' not in globals() or not BASE_EXPERIMENT_NAME:\n",
    "    print(\"❌ Error: BASE_EXPERIMENT_NAME not defined globally. Run Cell 1.\")\n",
    "    # No need to set run_folder_map as global if error\n",
    "    run_folder_map = {} # Define locally as empty dict\n",
    "else:\n",
    "    # Define the mapping from analysis label to the actual folder name\n",
    "    # These suffixes must match the EXPERIMENT_NAME set in Cell 1 of each run notebook (ablation_01-07)\n",
    "    run_folder_map = {\n",
    "        \"H+P (2D Ref)\": f\"{BASE_EXPERIMENT_NAME}_LinearHarmonicPheromone_REF\", # from ablation_01\n",
    "        \"P-Only (2D)\": f\"{BASE_EXPERIMENT_NAME}_LinearPheromoneOnly\",         # from ablation_02\n",
    "        \"H-Only (2D)\": f\"{BASE_EXPERIMENT_NAME}_LinearHarmonicOnly\",           # from ablation_03\n",
    "        \"H+3D-PH (Coupled)\": f\"{BASE_EXPERIMENT_NAME}_LinearHarmonic_PlaceholderDim3D\", # from ablation_04\n",
    "        \"H+5D-PH (Coupled)\": f\"{BASE_EXPERIMENT_NAME}_LinearHarmonic_PlaceholderDim5D\", # from ablation_05\n",
    "        \"H+5D-PH (Decoupled)\": f\"{BASE_EXPERIMENT_NAME}_LinearHarmonic_PlaceholderDim5D_DecoupledDiff\", # from ablation_06\n",
    "        \"H+4D-Bio (AIFM1)\": f\"{BASE_EXPERIMENT_NAME}_Harmonic4DBio\"           # from ablation_07\n",
    "    }\n",
    "\n",
    "    # --- MODIFIED: Set run_folder_map as a global variable ---\n",
    "    globals()['run_folder_map'] = run_folder_map\n",
    "    # --- END MODIFIED ---\n",
    "\n",
    "    print(\"Defined mapping of run labels to expected folder names for dynamic analysis:\")\n",
    "    for label, folder in run_folder_map.items():\n",
    "        print(f\"  '{label}': '{folder}'\")\n",
    "\n",
    "print(\"\\n ✅ Cell 3: Run folders defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643ffa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 4: Calculate Dynamic Region for Each Run (Loads Histories) (2025-04-28 21:18:27) ---\n",
      "Calculating dynamic region metric and identifying dynamic nodes for each run...\n",
      "  Processing 'H+P (2D Ref)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonicPheromone_REF)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.331264 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H+P (2D Ref).txt\n",
      "  Processing 'P-Only (2D)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearPheromoneOnly)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 0.000735 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_P-Only (2D).txt\n",
      "  Processing 'H-Only (2D)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonicOnly)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.321709 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H-Only (2D).txt\n",
      "  Processing 'H+3D-PH (Coupled)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim3D)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.323494 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H+3D-PH (Coupled).txt\n",
      "  Processing 'H+5D-PH (Coupled)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim5D)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.317422 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H+5D-PH (Coupled).txt\n",
      "  Processing 'H+5D-PH (Decoupled)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_LinearHarmonic_PlaceholderDim5D_DecoupledDiff)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.332778 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H+5D-PH (Decoupled).txt\n",
      "  Processing 'H+4D-Bio (AIFM1)' (Folder: string_ca_subgraph_AIFM1_CORRECTED_Harmonic4DBio)...\n",
      "    ✅ Identified 467 dynamic nodes (Threshold: 1.244398 percentile )\n",
      "    ✅ Saved dynamic region nodes to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_region_nodes_H+4D-Bio (AIFM1).txt\n",
      "\n",
      "Finished processing dynamic regions for all runs.\n",
      "\n",
      "Cell 4: Dynamic Region calculation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Calculate Dynamic Region for Each Run (Loads Histories)\n",
    "# Description: Iterates through the defined run folders and loads the Activation\n",
    "#              and Inhibition history DataFrames, calculates the dynamic region metric,\n",
    "#              and identifies the dynamic nodes based on the percentile threshold.\n",
    "#              Stores the results.\n",
    "#              Requires run folder map (Cell 3) and helper functions (Cell 1.1).\n",
    "#              COPIED from ablation_09 Cell 4.\n",
    "#              MODIFIED: Corrected the directory path used to load the history files.\n",
    "#              FIXED: Ensure run_dynamic_sizes, run_dynamic_metrics, run_dynamic_thresholds,\n",
    "#                     and run_dynamic_nodes_lists are stored globally *at the end of Cell 4*\n",
    "#                     so they are available for Cell 7 and other subsequent cells.\n",
    "#              FIXED: Ensure the *list of dynamic region node IDs* for each run is stored\n",
    "#                     in the `loaded_dynamic_regions` dictionary, as expected by Cell 5.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np # Needed for np.nan, np.percentile, np.where, np.nan_to_num\n",
    "# Helper functions load_history_dfs, calculate_time_avg_abs_change_metric, identify_dynamic_nodes defined in Cell 1.1\n",
    "\n",
    "print(f\"\\n--- Cell 4: Calculate Dynamic Region for Each Run (Loads Histories) ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "dynamic_region_calc_error = False\n",
    "\n",
    "# Check run folder map (from Cell 3)\n",
    "if 'run_folder_map' not in globals() or not isinstance(run_folder_map, dict) or not run_folder_map:\n",
    "    print(\"❌ Dynamic Region Calc Error: 'run_folder_map' is missing or invalid (Run Cell 3).\"); dynamic_region_calc_error = True\n",
    "\n",
    "# --- Check dynamic analysis output directory (from Cell 1) ---\n",
    "# This should be the output directory of ablation_09\n",
    "if 'INPUT_DIR_DYNAMIC_REGIONS' not in globals() or not INPUT_DIR_DYNAMIC_REGIONS:\n",
    "     print(\"❌ Dynamic Region Calc Error: Dynamic analysis input directory global 'INPUT_DIR_DYNAMIC_REGIONS' missing (Run Cell 1).\"); dynamic_region_calc_error = True\n",
    "elif not os.path.isdir(INPUT_DIR_DYNAMIC_REGIONS):\n",
    "     print(f\"❌ Dynamic Region Calc Error: Dynamic analysis input directory not found: {INPUT_DIR_DYNAMIC_REGIONS}. Ensure ablation_09 was run and created this directory.\"); dynamic_region_calc_error = True\n",
    "# --- END Corrected Check ---\n",
    "\n",
    "\n",
    "# Check history loading helper (Cell 1.1)\n",
    "if 'load_history_dfs' not in globals() or not callable(load_history_dfs):\n",
    "    print(\"❌ Dynamic Region Calc Error: History loading function 'load_history_dfs' missing (Defined in Cell 1.1?).\"); dynamic_region_calc_error = True\n",
    "# Check metric calculation helper (Cell 1.1)\n",
    "if 'calculate_time_avg_abs_change_metric' not in globals() or not callable(calculate_time_avg_abs_change_metric):\n",
    "    print(\"❌ Dynamic Region Calc Error: Metric calculation function missing (Defined in Cell 1.1?).\"); dynamic_region_calc_error = True\n",
    "# Check node identification helper (Cell 1.1)\n",
    "if 'identify_dynamic_nodes' not in globals() or not callable(identify_dynamic_nodes):\n",
    "    print(\"❌ Dynamic Region Calc Error: Node identification function missing (Defined in Cell 1.1?).\"); dynamic_region_calc_error = True\n",
    "# Check dynamic region parameters (Cell 1 - accessed via GLOBALS by helpers)\n",
    "if 'DYNAMIC_WINDOW_FRACTION' not in globals(): print(\"❌ Dynamic Region Calc Error: Required global 'DYNAMIC_WINDOW_FRACTION' missing (Run Cell 1).\"); dynamic_region_calc_error = True\n",
    "if 'DYNAMIC_THRESHOLD_VALUE' not in globals(): print(\"❌ Dynamic Region Calc Error: Required global 'DYNAMIC_THRESHOLD_VALUE' missing (Run Cell 1).\"); dynamic_region_calc_error = True\n",
    "if 'DYNAMIC_THRESHOLD_TYPE' not in globals(): print(\"❌ Dynamic Region Calc Error: Required global 'DYNAMIC_THRESHOLD_TYPE' missing (Run Cell 1).\"); dynamic_region_calc_error = True\n",
    "if 'DYNAMIC_METRIC_KEY' not in globals(): print(\"❌ Dynamic Region Calc Error: Required global 'DYNAMIC_METRIC_KEY' missing (Run Cell 1).\"); dynamic_region_calc_error = True # Needed by metric func\n",
    "\n",
    "\n",
    "# Check output directory for saving dynamic region node lists (from Cell 1)\n",
    "if 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' not in globals() or not OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS:\n",
    "     print(\"❌ Dynamic Region Calc Error: Output directory global 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' missing (Run Cell 1).\"); dynamic_region_calc_error = True\n",
    "elif not os.path.isdir(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS):\n",
    "     print(f\"❌ Dynamic Region Calc Error: Output directory not found: {OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS}. Check Cell 1.\"); dynamic_region_calc_error = True\n",
    "\n",
    "\n",
    "# --- Initialize dictionaries to store results ---\n",
    "# MODIFIED: Initialize the dictionaries that will be stored globally\n",
    "run_dynamic_metrics = {} # {run_label: pandas Series of metric values per node}\n",
    "run_dynamic_nodes_lists = {} # {run_label: list of dynamic node IDs}\n",
    "run_dynamic_thresholds = {} # {run_label: actual threshold value used}\n",
    "run_dynamic_sizes = {} # {run_label: number of dynamic nodes}\n",
    "# --- Added: Initialize the dictionary expected by Cell 5 ---\n",
    "loaded_dynamic_regions = {} # {run_label: list of dynamic node IDs} - This is what Cell 5 expects\n",
    "# END MODIFIED\n",
    "\n",
    "\n",
    "# --- Execute Dynamic Region Calculation ---\n",
    "if not dynamic_region_calc_error:\n",
    "    print(\"Calculating dynamic region metric and identifying dynamic nodes for each run...\")\n",
    "\n",
    "    for label, folder_name in run_folder_map.items():\n",
    "        print(f\"  Processing '{label}' (Folder: {folder_name})...\")\n",
    "\n",
    "        # --- Load history for this run ---\n",
    "        # MODIFIED: Load history from the correct directory (OUTPUT_DIR_SIMULATIONS)\n",
    "        act_df, inh_df = load_history_dfs(folder_name, OUTPUT_DIR_SIMULATIONS)\n",
    "        # END MODIFIED\n",
    "\n",
    "        if act_df is None or inh_df is None:\n",
    "            print(f\"    Skipping '{label}': History DataFrames not available or invalid.\")\n",
    "            run_dynamic_metrics[label] = None\n",
    "            run_dynamic_nodes_lists[label] = []\n",
    "            loaded_dynamic_regions[label] = [] # Store empty list for Cell 5\n",
    "            run_dynamic_thresholds[label] = np.nan # Use NaN for numeric threshold\n",
    "            run_dynamic_sizes[label] = 0\n",
    "            continue\n",
    "\n",
    "        # --- Calculate the Dynamic Metric for this run ---\n",
    "        # The helper accesses parameters from GLOBALS\n",
    "        node_metric_values = calculate_time_avg_abs_change_metric(act_df, inh_df)\n",
    "\n",
    "        if node_metric_values is None:\n",
    "            print(f\"    Skipping '{label}': Metric calculation failed or resulted in None.\")\n",
    "            run_dynamic_metrics[label] = None\n",
    "            run_dynamic_nodes_lists[label] = []\n",
    "            loaded_dynamic_regions[label] = [] # Store empty list for Cell 5\n",
    "            run_dynamic_thresholds[label] = np.nan\n",
    "            run_dynamic_sizes[label] = 0\n",
    "            continue\n",
    "        if node_metric_values.empty:\n",
    "             print(f\"    Skipping '{label}': Metric calculation resulted in an empty Series.\")\n",
    "             run_dynamic_metrics[label] = node_metric_values # Store empty series\n",
    "             run_dynamic_nodes_lists[label] = []\n",
    "             loaded_dynamic_regions[label] = [] # Store empty list for Cell 5\n",
    "             run_dynamic_thresholds[label] = np.nan\n",
    "             run_dynamic_sizes[label] = 0\n",
    "             continue\n",
    "\n",
    "\n",
    "        run_dynamic_metrics[label] = node_metric_values # Store the metric values Series\n",
    "\n",
    "        # --- Identify Dynamic Nodes for this run ---\n",
    "        # The helper accesses threshold parameters from GLOBALS\n",
    "        dynamic_nodes_list = identify_dynamic_nodes(node_metric_values)\n",
    "\n",
    "        run_dynamic_nodes_lists[label] = dynamic_nodes_list # Store the list of node IDs\n",
    "        # --- Added: Store the list in the variable expected by Cell 5 ---\n",
    "        loaded_dynamic_regions[label] = dynamic_nodes_list\n",
    "        # --- End Added ---\n",
    "        run_dynamic_sizes[label] = len(dynamic_nodes_list) # Store the size\n",
    "\n",
    "        # --- Store the *actual* threshold used for this run ---\n",
    "        # Need to recalculate it here because identify_dynamic_nodes returns just the list, not the threshold\n",
    "        actual_threshold_for_run = np.nan # Initialize\n",
    "        valid_metrics = node_metric_values.dropna().values # Exclude NaNs for threshold calc\n",
    "        if len(valid_metrics) > 0:\n",
    "             threshold_type = globals().get('DYNAMIC_THRESHOLD_TYPE', 'percentile')\n",
    "             threshold_value = globals().get('DYNAMIC_THRESHOLD_VALUE', 80)\n",
    "             try:\n",
    "                  if threshold_type == 'percentile':\n",
    "                       # Ensure value is within bounds for percentile\n",
    "                       percentile_val = threshold_value\n",
    "                       if not (0 <= percentile_val <= 100): percentile_val = 80 # Default if invalid\n",
    "\n",
    "                       actual_threshold_for_run = np.percentile(valid_metrics, percentile_val)\n",
    "                  elif threshold_type == 'absolute':\n",
    "                       actual_threshold_for_run = threshold_value\n",
    "                  else: warnings.warn(f\"    Unknown threshold type in globals: '{threshold_type}'. Threshold will be NaN.\")\n",
    "             except Exception as e_thresh_calc: warnings.warn(f\"    Error calculating threshold for '{label}': {e_thresh_calc}. Threshold will be NaN.\");\n",
    "\n",
    "        run_dynamic_thresholds[label] = actual_threshold_for_run # Store the calculated/used threshold\n",
    "\n",
    "        print(f\"    ✅ Identified {run_dynamic_sizes[label]} dynamic nodes (Threshold: {actual_threshold_for_run:.6f} {globals().get('DYNAMIC_THRESHOLD_TYPE','')} )\")\n",
    "\n",
    "        # --- Save Dynamic Region Nodes to File ---\n",
    "        if dynamic_nodes_list:\n",
    "             # Save to the correct output directory for this notebook (OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS)\n",
    "             dynamic_region_filename = os.path.join(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, f\"dynamic_region_nodes_{label}.txt\") # Use label in filename\n",
    "             try:\n",
    "                  with open(dynamic_region_filename, 'w') as f:\n",
    "                       for node_id in dynamic_nodes_list: f.write(f\"{node_id}\\n\")\n",
    "                  print(f\"    ✅ Saved dynamic region nodes to: {dynamic_region_filename}\")\n",
    "             except Exception as e: print(f\"    ❌ Error saving dynamic region node list: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\nFinished processing dynamic regions for all runs.\")\n",
    "\n",
    "else: # dynamic_region_calc_error was True\n",
    "    print(\"Skipping dynamic region calculation due to missing prerequisites.\")\n",
    "\n",
    "\n",
    "# --- Store globally at the end of Cell 4 ---\n",
    "globals()['run_dynamic_metrics'] = run_dynamic_metrics # The metric values per node\n",
    "globals()['run_dynamic_nodes_lists'] = run_dynamic_nodes_lists # The lists of identified nodes\n",
    "globals()['run_dynamic_thresholds'] = run_dynamic_thresholds # The actual thresholds used\n",
    "globals()['run_dynamic_sizes'] = run_dynamic_sizes # The size of the dynamic region\n",
    "# --- Added: Store the variable expected by Cell 5 ---\n",
    "globals()['loaded_dynamic_regions'] = loaded_dynamic_regions\n",
    "# --- End Added ---\n",
    "\n",
    "\n",
    "print(\"\\nCell 4: Dynamic Region calculation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2650c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 5: Map Dynamic Region Nodes to Gene Symbols (2025-04-28 21:18:30) ---\n",
      "Attempting to map dynamic region node IDs to gene symbols...\n",
      "  Mapping 467 nodes for 'H+P (2D Ref)'...\n",
      "Mapping 467 Dynamic Region 'H+P (2D Ref)' IDs...\n",
      "  🌐 Mapping 467 IDs via STRING API...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640c87b180b741c5b8c98123ee8fd067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "API Mapping (Dynamic Region 'H+P (2D Ref)'):   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Updated mapping cache saved (467 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H+P (2D Ref)'.\n",
      "  Mapping 467 nodes for 'P-Only (2D)'...\n",
      "Mapping 467 Dynamic Region 'P-Only (2D)' IDs...\n",
      "  ℹ️ Loaded 467 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 372 IDs via STRING API...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19434e993e04e658f48f2d196dda7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "API Mapping (Dynamic Region 'P-Only (2D)'):   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Updated mapping cache saved (839 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'P-Only (2D)'.\n",
      "  Mapping 467 nodes for 'H-Only (2D)'...\n",
      "Mapping 467 Dynamic Region 'H-Only (2D)' IDs...\n",
      "  ℹ️ Loaded 839 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 144 IDs via STRING API...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883487307d64879a9c44dfbaab251f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "API Mapping (Dynamic Region 'H-Only (2D)'):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Updated mapping cache saved (983 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H-Only (2D)'.\n",
      "  Mapping 467 nodes for 'H+3D-PH (Coupled)'...\n",
      "Mapping 467 Dynamic Region 'H+3D-PH (Coupled)' IDs...\n",
      "  ℹ️ Loaded 983 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 96 IDs via STRING API...\n",
      "  ✅ Updated mapping cache saved (1079 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H+3D-PH (Coupled)'.\n",
      "  Mapping 467 nodes for 'H+5D-PH (Coupled)'...\n",
      "Mapping 467 Dynamic Region 'H+5D-PH (Coupled)' IDs...\n",
      "  ℹ️ Loaded 1079 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 39 IDs via STRING API...\n",
      "  ✅ Updated mapping cache saved (1118 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H+5D-PH (Coupled)'.\n",
      "  Mapping 467 nodes for 'H+5D-PH (Decoupled)'...\n",
      "Mapping 467 Dynamic Region 'H+5D-PH (Decoupled)' IDs...\n",
      "  ℹ️ Loaded 1118 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 41 IDs via STRING API...\n",
      "  ✅ Updated mapping cache saved (1159 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H+5D-PH (Decoupled)'.\n",
      "  Mapping 467 nodes for 'H+4D-Bio (AIFM1)'...\n",
      "Mapping 467 Dynamic Region 'H+4D-Bio (AIFM1)' IDs...\n",
      "  ℹ️ Loaded 1159 mappings from cache: biological_analysis_results/Dynamic_Biological_Analysis/string_id_to_gene_symbol_map_cache.pkl\n",
      "  🌐 Mapping 22 IDs via STRING API...\n",
      "  ✅ Updated mapping cache saved (1181 total mappings).\n",
      "    ✅ Mapped to 467 gene symbols for 'H+4D-Bio (AIFM1)'.\n",
      "\n",
      "Finished attempting to map gene symbols for dynamic regions.\n",
      "\n",
      "Cell 5: Dynamic Region node mapping complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Map Dynamic Region Nodes to Gene Symbols\n",
    "# Description: Iterates through the loaded dynamic region node lists, maps the STRING IDs\n",
    "#              to gene symbols for each run's list using the helper function.\n",
    "#              Stores the resulting lists of gene symbols.\n",
    "#              Requires dynamic region lists (Cell 4) and helper function (Cell 2).\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd # Keep pandas import if needed later\n",
    "# Helper function map_string_ids_to_gene_symbols_api defined in Cell 2\n",
    "\n",
    "print(f\"\\n--- Cell 5: Map Dynamic Region Nodes to Gene Symbols ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "mapping_error = False\n",
    "# Check loaded dynamic regions (from Cell 4)\n",
    "if 'loaded_dynamic_regions' not in globals() or not isinstance(loaded_dynamic_regions, dict):\n",
    "    print(\"❌ Mapping Error: 'loaded_dynamic_regions' missing or invalid (Run Cell 4).\"); mapping_error = True\n",
    "elif not any(loaded_dynamic_regions.values()): # Check if *any* of the lists are non-empty\n",
    "    print(\"⚠️ Skipping Mapping: All dynamic region node lists loaded in Cell 4 are empty.\"); mapping_error = True\n",
    "# Check mapping helper function (Cell 2)\n",
    "if 'map_string_ids_to_gene_symbols_api' not in globals() or not callable(map_string_ids_to_gene_symbols_api):\n",
    "    print(\"❌ Mapping Error: Helper function 'map_string_ids_to_gene_symbols_api' missing (Defined in Cell 2?).\"); mapping_error = True\n",
    "# Check necessary config parameters (Cell 1 - accessed via GLOBALS by helper)\n",
    "if 'SPECIES_ID' not in globals(): print(\"❌ Mapping Error: Required global 'SPECIES_ID' missing (Run Cell 1).\"); mapping_error = True\n",
    "if 'TARGET_NODE_NAME' not in globals(): print(\"⚠️ Mapping Warning: Required global 'TARGET_NODE_NAME' missing (Run Cell 1). Description may be generic.\"); # Allow proceed\n",
    "if 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' not in globals() or not OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS:\n",
    "     print(\"❌ Mapping Error: Required global 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' missing (Run Cell 1).\"); mapping_error = True\n",
    "elif not os.path.isdir(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS):\n",
    "     print(f\"❌ Mapping Error: Output directory not found: {OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS}. Check Cell 1.\"); mapping_error = True\n",
    "\n",
    "\n",
    "# --- Initialize dictionary to store mapped gene symbols ---\n",
    "# {run_label: list of gene symbols}\n",
    "mapped_dynamic_regions_genes = {}\n",
    "genes_mapped_available = False # Flag if any lists were successfully mapped and non-empty\n",
    "\n",
    "# --- Execute Mapping ---\n",
    "if not mapping_error:\n",
    "    print(f\"Attempting to map dynamic region node IDs to gene symbols...\")\n",
    "\n",
    "    for label, node_list in loaded_dynamic_regions.items():\n",
    "        if not node_list:\n",
    "            print(f\"  Skipping mapping for '{label}': Node list is empty.\")\n",
    "            mapped_dynamic_regions_genes[label] = [] # Store empty list\n",
    "            continue\n",
    "\n",
    "        print(f\"  Mapping {len(node_list)} nodes for '{label}'...\")\n",
    "        try:\n",
    "            # Call the helper function defined in Cell 2\n",
    "            # It accesses config parameters from GLOBALS and uses the output dir of this notebook for cache\n",
    "            gene_symbols_list = map_string_ids_to_gene_symbols_api(\n",
    "                string_ids=node_list,\n",
    "                description=f\"Dynamic Region '{label}'\" # Use label in description\n",
    "            )\n",
    "\n",
    "            mapped_dynamic_regions_genes[label] = gene_symbols_list # Store the list of gene symbols\n",
    "\n",
    "            if gene_symbols_list:\n",
    "                 print(f\"    ✅ Mapped to {len(gene_symbols_list)} gene symbols for '{label}'.\")\n",
    "                 genes_mapped_available = True # Set flag if at least one list is non-empty\n",
    "            else:\n",
    "                 print(f\"    ⚠️ Mapping yielded no gene symbols for '{label}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ An error occurred during mapping for '{label}': {e}\")\n",
    "            traceback.print_exc()\n",
    "            mapped_dynamic_regions_genes[label] = [] # Store empty list on error\n",
    "            # Do not set fatal mapping_error, allow others to run\n",
    "\n",
    "\n",
    "    print(\"\\nFinished attempting to map gene symbols for dynamic regions.\")\n",
    "    if not genes_mapped_available:\n",
    "         warnings.warn(\"⚠️ No dynamic region lists were successfully mapped to gene symbols. Biological analysis will be skipped.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping gene symbol mapping due to previous errors or all input lists being empty\")\n",
    "\n",
    "# Store globally for subsequent cells\n",
    "globals()['mapped_dynamic_regions_genes'] = mapped_dynamic_regions_genes\n",
    "\n",
    "print(\"\\nCell 5: Dynamic Region node mapping complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32436ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 6: Perform Enrichment Analysis for Each Dynamic Region (2025-04-28 21:18:52) ---\n",
      "Attempting to perform GO enrichment for dynamic regions...\n",
      "  Running enrichment for 'H+P (2D Ref)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H+P (2D Ref)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2612 significant terms for 'H+P (2D Ref)'.\n",
      "  Running enrichment for 'P-Only (2D)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'P-Only (2D)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2544 significant terms for 'P-Only (2D)'.\n",
      "  Running enrichment for 'H-Only (2D)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H-Only (2D)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2560 significant terms for 'H-Only (2D)'.\n",
      "  Running enrichment for 'H+3D-PH (Coupled)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H+3D-PH (Coupled)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2512 significant terms for 'H+3D-PH (Coupled)'.\n",
      "  Running enrichment for 'H+5D-PH (Coupled)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H+5D-PH (Coupled)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2589 significant terms for 'H+5D-PH (Coupled)'.\n",
      "  Running enrichment for 'H+5D-PH (Decoupled)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H+5D-PH (Decoupled)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2703 significant terms for 'H+5D-PH (Decoupled)'.\n",
      "  Running enrichment for 'H+4D-Bio (AIFM1)' (467 genes)...\n",
      "--- 📊 Performing Enrichment: Dynamic Region 'H+4D-Bio (AIFM1)' (467 genes) using 'GO_Biological_Process_2023' ---\n",
      "    ✅ Found 2700 significant terms for 'H+4D-Bio (AIFM1)'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Perform Enrichment Analysis for Each Dynamic Region\n",
    "# Description: Iterates through the mapped gene symbol lists and performs GO BP\n",
    "#              enrichment analysis for each run's dynamic region. Stores the results.\n",
    "#              Requires mapped gene lists (Cell 5) and helper function (Cell 2).\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "# Helper function run_enrichment_simple defined in Cell 2\n",
    "\n",
    "# Ensure gseapy is available (checked in Cell 0 or earlier)\n",
    "try: import gseapy as gp # Use alias\n",
    "except ImportError: gp = None\n",
    "\n",
    "print(f\"\\n--- Cell 6: Perform Enrichment Analysis for Each Dynamic Region ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "enrichment_error = False\n",
    "# Check mapped gene lists (from Cell 5)\n",
    "if 'mapped_dynamic_regions_genes' not in globals() or not isinstance(mapped_dynamic_regions_genes, dict):\n",
    "    print(\"❌ Enrichment Error: 'mapped_dynamic_regions_genes' missing or invalid (Run Cell 5).\"); enrichment_error = True\n",
    "elif not any(mapped_dynamic_regions_genes.values()): # Check if *any* of the lists are non-empty\n",
    "    print(\"⚠️ Skipping Enrichment: All mapped gene symbol lists are empty.\"); enrichment_error = True\n",
    "# Check enrichment helper function (Cell 2)\n",
    "if 'run_enrichment_simple' not in globals() or not callable(run_enrichment_simple):\n",
    "    print(\"❌ Enrichment Error: Helper function 'run_enrichment_simple' missing (Defined in Cell 2?).\"); enrichment_error = True\n",
    "# Check gseapy availability (handled by import check and gp alias)\n",
    "if gp is None: print(\"❌ Enrichment Error: gseapy library not available.\"); enrichment_error = True\n",
    "# Check necessary config parameters (Cell 1 - accessed via GLOBALS by helper)\n",
    "if 'GO_ENRICHMENT_LIBRARY' not in globals(): print(\"❌ Enrichment Error: Required global 'GO_ENRICHMENT_LIBRARY' missing (Run Cell 1).\"); enrichment_error = True\n",
    "if 'TOP_N_ENRICHMENT_TERMS' not in globals(): print(\"⚠️ Enrichment Warning: Required global 'TOP_N_ENRICHMENT_TERMS' missing (Run Cell 1). Display may be default.\"); # Allow proceed\n",
    "if 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' not in globals() or not OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS:\n",
    "     print(\"❌ Enrichment Error: Required global 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' missing (Run Cell 1).\"); enrichment_error = True\n",
    "elif not os.path.isdir(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS):\n",
    "     print(f\"❌ Enrichment Error: Output directory not found: {OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS}. Check Cell 1.\"); enrichment_error = True\n",
    "\n",
    "\n",
    "# --- Initialize dictionary to store enrichment results ---\n",
    "# {run_label: pandas DataFrame of enrichment results (can be empty if no sig terms)}\n",
    "dynamic_regions_enrichment_results = {}\n",
    "\n",
    "# --- Execute Enrichment ---\n",
    "if not enrichment_error:\n",
    "    print(f\"Attempting to perform GO enrichment for dynamic regions...\")\n",
    "\n",
    "    for label, gene_list in mapped_dynamic_regions_genes.items():\n",
    "        if not gene_list or len(gene_list) < 3:\n",
    "            print(f\"  Skipping enrichment for '{label}': Fewer than 3 genes ({len(gene_list)}).\")\n",
    "            dynamic_regions_enrichment_results[label] = pd.DataFrame() # Store empty DF\n",
    "            continue\n",
    "\n",
    "        print(f\"  Running enrichment for '{label}' ({len(gene_list)} genes)...\")\n",
    "        try:\n",
    "            # Call the helper function defined in Cell 2\n",
    "            # It accesses config parameters (library, top_n) from GLOBALS\n",
    "            results_df = run_enrichment_simple(\n",
    "                gene_list=gene_list,\n",
    "                description=f\"Dynamic Region '{label}'\" # Use label in description\n",
    "            )\n",
    "\n",
    "            # Store the results DataFrame (can be empty if no significant terms)\n",
    "            if isinstance(results_df, pd.DataFrame):\n",
    "                 dynamic_regions_enrichment_results[label] = results_df\n",
    "                 if results_df.empty:\n",
    "                      print(f\"    ℹ️ No significant terms found for '{label}'.\")\n",
    "                 else:\n",
    "                      print(f\"    ✅ Found {len(results_df)} significant terms for '{label}'.\")\n",
    "                      # Optional: Save full results for this run\n",
    "                      enr_filename = os.path.join(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, f\"dynamic_region_{label}_enrichment_results.csv\")\n",
    "                      try:\n",
    "                           results_df.to_csv(enr_filename, index=False)\n",
    "                      except Exception as e_save: print(f\"    ❌ Error saving enrichment results CSV for '{label}': {e_save}\")\n",
    "\n",
    "            elif results_df is None:\n",
    "                 print(f\"    ❌ Enrichment function returned None for '{label}'.\")\n",
    "                 dynamic_regions_enrichment_results[label] = pd.DataFrame() # Store empty DF on None return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ An unexpected error occurred during enrichment for '{label}': {e}\")\n",
    "            traceback.print_exc()\n",
    "            dynamic_regions_enrichment_results[label] = pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de3ef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 7: Compile and Format Comparative Enrichment Table (2025-04-28 21:19:11) ---\n",
      "  ✅ Loaded dynamic analysis comparison table from: biological_analysis_results/Dynamic_Analysis_Across_Runs/dynamic_analysis_comparison_table.csv\n",
      "Compiling data for comparative enrichment table...\n",
      "✅ Comparative enrichment table DataFrame created and formatted.\n",
      "✅ Saved comparative enrichment table to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_biological_enrichment_comparison_table.csv\n",
      "\n",
      "✅ Cell 7: Comparative enrichment table creation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Compile and Format Comparative Enrichment Table\n",
    "# Description: Gathers key enrichment metrics (number of terms, top term, p-value)\n",
    "#              for the dynamic region of each run and compiles them into a pandas DataFrame.\n",
    "#              Formats the DataFrame for presentation in the final markdown summary.\n",
    "#              Requires dynamic region enrichment results (Cell 6) and the dynamic analysis table (from ablation_09).\n",
    "#              MODIFIED: Compiles the main comparative *enrichment* table by merging data from Cell 6 and the ablation_09 output file.\n",
    "#              MODIFIED: Added explicit saving of the resulting DataFrame to a CSV file.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "print(f\"\\n--- Cell 7: Compile and Format Comparative Enrichment Table ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "table_creation_error = False\n",
    "\n",
    "# Check enrichment results DataFrame from Cell 6 of THIS notebook\n",
    "if 'dynamic_regions_enrichment_results' not in globals() or not isinstance(dynamic_regions_enrichment_results, dict):\n",
    "    print(\"❌ Table Creation Error: 'dynamic_regions_enrichment_results' missing or invalid (Run Cell 6).\"); table_creation_error = True\n",
    "# Note: It's okay if the dict is empty or contains empty DataFrames; the table will reflect that.\n",
    "\n",
    "# Check the dynamic analysis comparison table (from ablation_09 output file)\n",
    "# We will *load* this file here, so check the file path global (from Cell 1)\n",
    "if 'OUTPUT_DIR_DYNAMIC_ANALYSIS' not in globals() or not OUTPUT_DIR_DYNAMIC_ANALYSIS:\n",
    "     print(\"❌ Table Creation Error: Dynamic analysis output directory global 'OUTPUT_DIR_DYNAMIC_ANALYSIS' missing (Run Cell 1). Cannot load dynamic analysis table.\"); table_creation_error = True\n",
    "else:\n",
    "     # Attempt to load the dynamic analysis comparison table CSV\n",
    "     dynamic_analysis_table_path = os.path.join(OUTPUT_DIR_DYNAMIC_ANALYSIS, \"dynamic_analysis_comparison_table.csv\") # Assumed filename from ablation_09\n",
    "     dynamic_analysis_df_loaded = pd.DataFrame() # Initialize empty\n",
    "     try:\n",
    "          if os.path.exists(dynamic_analysis_table_path):\n",
    "               dynamic_analysis_df_loaded = pd.read_csv(dynamic_analysis_table_path, index_col=0)\n",
    "               print(f\"  ✅ Loaded dynamic analysis comparison table from: {dynamic_analysis_table_path}\")\n",
    "               if dynamic_analysis_df_loaded.empty: warnings.warn(\"  ⚠️ Loaded dynamic analysis comparison table is empty.\")\n",
    "          else:\n",
    "               print(f\"❌ Table Creation Error: Dynamic analysis comparison table not found at: {dynamic_analysis_table_path}. Ensure ablation_09 was run and completed successfully.\");\n",
    "               table_creation_error = True # Critical error if this file is missing\n",
    "\n",
    "     except Exception as e_load_dynamic_analysis:\n",
    "          print(f\"❌ Error loading dynamic analysis comparison table: {e_load_dynamic_analysis}\"); traceback.print_exc();\n",
    "          table_creation_error = True # Critical error\n",
    "\n",
    "# Ensure we have valid data to merge, even if one source is empty\n",
    "if not table_creation_error and not (dynamic_regions_enrichment_results or not dynamic_analysis_df_loaded.empty):\n",
    "     print(\"⚠️ Skipping Table Creation: No run data available from either enrichment results (Cell 6) or dynamic analysis table (ablation_09 output).\")\n",
    "     table_creation_error = True # No data to process\n",
    "\n",
    "\n",
    "# --- Compile Data for the Comparative Enrichment Table ---\n",
    "# This table will combine summary enrichment metrics with dynamic analysis metrics for each run\n",
    "enrichment_comparison_table_data = {} # {run_label: {metric_name: value}}\n",
    "\n",
    "if not table_creation_error:\n",
    "    print(\"Compiling data for comparative enrichment table...\")\n",
    "\n",
    "    # Get all run labels from the enrichment results dictionary AND the dynamic analysis table\n",
    "    # This ensures all runs are included if they appeared in either source\n",
    "    all_run_labels = sorted(list(dynamic_regions_enrichment_results.keys()) + list(dynamic_analysis_df_loaded.index))\n",
    "    all_run_labels = sorted(list(set(all_run_labels))) # Get unique and sort\n",
    "\n",
    "    if not all_run_labels:\n",
    "        print(\"⚠️ No run labels found to compile data for the enrichment table.\")\n",
    "        table_creation_error = True # Nothing to put in the table\n",
    "\n",
    "if not table_creation_error:\n",
    "    for label in all_run_labels:\n",
    "        enrichment_comparison_table_data[label] = {} # Initialize entry for this run\n",
    "\n",
    "        # --- Add Enrichment Metrics (from Cell 6 results) ---\n",
    "        enrichment_df = dynamic_regions_enrichment_results.get(label)\n",
    "\n",
    "        if isinstance(enrichment_df, pd.DataFrame):\n",
    "            if not enrichment_df.empty:\n",
    "                # Enrichment ran and found significant terms\n",
    "                num_terms = len(enrichment_df)\n",
    "\n",
    "                # Safely get Top Term and Top Term Adj P from the DataFrame\n",
    "                top_term = enrichment_df.iloc[0].get('Term', 'N/A')\n",
    "                top_pval = enrichment_df.iloc[0].get('Adjusted P-value', np.nan) # Use get with NaN default\n",
    "\n",
    "                enrichment_comparison_table_data[label]['Significant Terms'] = num_terms\n",
    "                enrichment_comparison_table_data[label]['Top Term'] = top_term\n",
    "                enrichment_comparison_table_data[label]['Top Term Adj P'] = top_pval\n",
    "\n",
    "            else:\n",
    "                # Enrichment ran but found no significant terms (empty DF)\n",
    "                enrichment_comparison_table_data[label]['Significant Terms'] = 0\n",
    "                enrichment_comparison_table_data[label]['Top Term'] = \"No significant terms\"\n",
    "                enrichment_comparison_table_data[label]['Top Term Adj P'] = np.nan # Use NaN for p-value\n",
    "\n",
    "        else: # Enrichment did not run or failed for this specific run (e.g., returned None)\n",
    "            # Add placeholder values indicating the analysis was skipped or failed for this run\n",
    "            enrichment_comparison_table_data[label]['Significant Terms'] = \"Error/Skipped\"\n",
    "            enrichment_comparison_table_data[label]['Top Term'] = \"Error/Skipped\"\n",
    "            enrichment_comparison_table_data[label]['Top Term Adj P'] = np.nan # Use NaN for p-value\n",
    "\n",
    "\n",
    "        # --- Add Dynamic Analysis Metrics (from ablation_09 output) ---\n",
    "        # Get row from the loaded dynamic analysis DataFrame, default to empty Series if label not in index\n",
    "        dynamic_analysis_row = dynamic_analysis_df_loaded.loc[label] if label in dynamic_analysis_df_loaded.index else pd.Series(dtype=object) # Use object dtype for mixed types\n",
    "\n",
    "        # Populate table entry with dynamic analysis metrics, defaulting to N/A or NaN if column/row is missing\n",
    "        enrichment_comparison_table_data[label]['Dynamic Region Size'] = dynamic_analysis_row.get('Dynamic Region Size', np.nan)\n",
    "        enrichment_comparison_table_data[label]['Mapped Genes'] = dynamic_analysis_row.get('Mapped Genes', np.nan) # Mapped Genes is calculated in THIS notebook\n",
    "        enrichment_comparison_table_data[label]['Dynamic Metric Threshold'] = dynamic_analysis_row.get('Dynamic Metric Threshold', np.nan)\n",
    "        enrichment_comparison_table_data[label][f'{globals().get(\"DYNAMIC_METRIC_KEY\", \"Metric\")} (Avg in Region)'] = dynamic_analysis_row.get(f'{globals().get(\"DYNAMIC_METRIC_NAME\", \"Dynamic Metric\")} (Avg in Region)', np.nan)\n",
    "\n",
    "        # Add Jaccard columns\n",
    "        jaccard_cols_prefix = 'Jaccard vs '\n",
    "        # Find all Jaccard columns in the loaded dynamic analysis DataFrame\n",
    "        jaccard_cols_in_loaded_df = [col for col in dynamic_analysis_df_loaded.columns if col.startswith(jaccard_cols_prefix)]\n",
    "        for jaccard_col in jaccard_cols_in_loaded_df:\n",
    "             enrichment_comparison_table_data[label][jaccard_col] = dynamic_analysis_row.get(jaccard_col, np.nan)\n",
    "\n",
    "\n",
    "    # --- Create pandas DataFrame from compiled data ---\n",
    "    # Define desired column order for the final table\n",
    "    # Ensure dynamic analysis columns are included in the order\n",
    "    column_order = [\n",
    "        'Dynamic Region Size',\n",
    "        'Mapped Genes', # Mapped Genes is from THIS notebook (Cell 5's output), added here\n",
    "        'Significant Terms',\n",
    "        'Top Term',\n",
    "        'Top Term Adj P',\n",
    "        'Dynamic Metric Threshold',\n",
    "        f'{globals().get(\"DYNAMIC_METRIC_KEY\", \"Metric\")} (Avg in Region)',\n",
    "        # Add Jaccard columns dynamically based on the headers found\n",
    "    ]\n",
    "    # Add Jaccard columns found in the loaded dynamic analysis DF to the end of the order\n",
    "    column_order.extend(sorted(jaccard_cols_in_loaded_df)) # Add and sort Jaccard columns\n",
    "\n",
    "    # Create DataFrame directly from the compiled dictionary\n",
    "    # This will automatically handle columns that might be missing for some rows (they'll be NaN)\n",
    "    final_enrichment_comparison_df = pd.DataFrame.from_dict(enrichment_comparison_table_data, orient='index')\n",
    "\n",
    "    # Reindex the DataFrame to enforce column order, dropping any columns not in the list\n",
    "    existing_and_ordered_cols = [col for col in column_order if col in final_enrichment_comparison_df.columns]\n",
    "    final_enrichment_comparison_df = final_enrichment_comparison_df[existing_and_ordered_cols].copy()\n",
    "\n",
    "\n",
    "    # --- Format numeric columns for display ---\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = final_enrichment_comparison_df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # Apply formatting, handling NaN values gracefully\n",
    "        # Format as integer or float with N/A for NaN\n",
    "        if col in ['Dynamic Region Size', 'Mapped Genes', 'Significant Terms']:\n",
    "            # Format as integer, handling potential NaN/non-numeric\n",
    "             final_enrichment_comparison_df[col] = final_enrichment_comparison_df[col].apply(lambda x: int(x) if pd.notna(x) and isinstance(x, (int, float)) else x)\n",
    "        elif col == 'Top Term Adj P':\n",
    "            # Apply formatting for p-value (exponential), handling NaN/non-numeric\n",
    "             final_enrichment_comparison_df[col] = final_enrichment_comparison_df[col].apply(lambda x: f\"{x:.2e}\" if pd.notna(x) and isinstance(x, (int, float, np.number)) else \"N/A\")\n",
    "        else: # Default formatting for other floats (threshold, average metric, Jaccard)\n",
    "             final_enrichment_comparison_df[col] = final_enrichment_comparison_df[col].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) and isinstance(x, (int, float, np.number)) else \"N/A\")\n",
    "\n",
    "\n",
    "    print(\"✅ Comparative enrichment table DataFrame created and formatted.\")\n",
    "\n",
    "\n",
    "    # --- MODIFIED: Explicitly save the compiled DataFrame to CSV ---\n",
    "    table_output_filename = os.path.join(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, \"dynamic_biological_enrichment_comparison_table.csv\")\n",
    "    try:\n",
    "        # Save with index=True to keep the run labels as the first column\n",
    "        final_enrichment_comparison_df.to_csv(table_output_filename, index=True)\n",
    "        print(f\"✅ Saved comparative enrichment table to: {table_output_filename}\")\n",
    "    except Exception as e_save_csv:\n",
    "        print(f\"❌ Error saving comparative enrichment table to CSV: {e_save_csv}\")\n",
    "        traceback.print_exc()\n",
    "        table_creation_error = True # Flag error if saving failed\n",
    "    # --- END MODIFIED ---\n",
    "\n",
    "\n",
    "else: # table_creation_error was True\n",
    "    print(\"Skipping comparative enrichment table creation due to missing prerequisites or no run data.\")\n",
    "    final_enrichment_comparison_df = pd.DataFrame() # Ensure empty DF on error\n",
    "\n",
    "\n",
    "# Store globally for subsequent cells\n",
    "globals()['dynamic_biological_analysis_comparison_df'] = final_enrichment_comparison_df\n",
    "\n",
    "\n",
    "print(\"\\n✅ Cell 7: Comparative enrichment table creation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23465393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 8: Generate Dynamic Biological Analysis Summary Markdown (2025-04-28 21:19:11) ---\n",
      "✅ Dynamic biological analysis summary markdown generated.\n",
      "\n",
      "Cell 8: Dynamic biological analysis summary markdown generation complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Generate Dynamic Biological Analysis Summary Markdown\n",
    "# Description: Generates the markdown text for the biological analysis summary,\n",
    "#              including the comparative enrichment table and interpretation.\n",
    "\n",
    "import pandas as pd # Needed for to_markdown\n",
    "import time\n",
    "# Access global dynamic_biological_analysis_comparison_df from Cell 7\n",
    "\n",
    "print(f\"\\n--- Cell 8: Generate Dynamic Biological Analysis Summary Markdown ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "markdown_gen_error = False\n",
    "if 'dynamic_biological_analysis_comparison_df' not in globals() or not isinstance(dynamic_biological_analysis_comparison_df, pd.DataFrame):\n",
    "    print(\"❌ Markdown Generation Error: 'dynamic_biological_analysis_comparison_df' missing or invalid (Run Cell 7).\"); markdown_gen_error = True\n",
    "\n",
    "# --- Generate Markdown Text ---\n",
    "dynamic_biological_analysis_summary_markdown = \"\"\n",
    "\n",
    "if not markdown_gen_error:\n",
    "    if dynamic_biological_analysis_comparison_df.empty:\n",
    "        print(\"⚠️ Cannot generate markdown table: Comparative enrichment DataFrame is empty.\")\n",
    "        comparison_table_md = \"*(No data available to generate table)*\"\n",
    "    else:\n",
    "        try:\n",
    "            comparison_table_md = dynamic_biological_analysis_comparison_df.to_markdown(numalign='left', stralign='left')\n",
    "        except ImportError:\n",
    "            comparison_table_md = \"*(Table generation failed: 'tabulate' library missing. Install it.)*\"\n",
    "        except Exception as e_table:\n",
    "            print(f\"❌ Error converting DataFrame to markdown table: {e_table}\")\n",
    "            comparison_table_md = f\"*(Table generation failed: {e_table})*\"\n",
    "\n",
    "    try:\n",
    "        target_name = globals().get('TARGET_NODE_NAME', 'Target Protein')\n",
    "\n",
    "        summary_text_lines = [f\"# Biological Relevance of Dynamic Regions Across Ruleset Ablations ({target_name} Subgraph)\\n\"]\n",
    "        summary_text_lines.append(\"## 1. Introduction\")\n",
    "        summary_text_lines.append(\"This analysis investigates the biological relevance of the **dynamically active regions** identified in simulations of various Network Automaton rulesets applied to the AIFM1 subgraph. By performing functional enrichment analysis on the gene sets corresponding to these dynamic regions, we assess which ruleset components contribute to generating dynamically organized patterns that are associated with known biological functions.\")\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"The dynamically active regions were identified using a consistent metric (time-averaged absolute state change over the final 20% window) and thresholded at the 80th percentile across all nodes.\")\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"## 2. Comparative Enrichment Analysis Table\")\n",
    "        summary_text_lines.append(\"The table below summarizes the functional enrichment results (GO Biological Process) for the dynamic region identified in each simulation run:\")\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(comparison_table_md)\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"_**Table Columns:**_\")\n",
    "        summary_text_lines.append(\"- **Dynamic Region Size:** Number of nodes in the dynamic region for that run.\")\n",
    "        summary_text_lines.append(\"- **Mapped Genes:** Number of unique gene symbols mapped from the dynamic region nodes.\")\n",
    "        summary_text_lines.append(\"- **Significant Terms:** Number of GO BP terms found to be significantly enriched (Adj. P < 0.05).\")\n",
    "        summary_text_lines.append(\"- **Top Term:** The GO BP term with the most significant adjusted P-value.\")\n",
    "        summary_text_lines.append(\"- **Top Term Adj P:** The adjusted P-value for the Top Term.\")\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"## 3. Interpretation of Biological Relevance\")\n",
    "        summary_text_lines.append(\"Based on the comparative enrichment analysis:\")\n",
    "        summary_text_lines.append(\"\")\n",
    "\n",
    "        if 'H+P (2D Ref)' in dynamic_biological_analysis_comparison_df.index:\n",
    "             hp_sig_terms = dynamic_biological_analysis_comparison_df.loc['H+P (2D Ref)'].get('Significant Terms', 0)\n",
    "             hp_top_term = dynamic_biological_analysis_comparison_df.loc['H+P (2D Ref)'].get('Top Term', 'N/A')\n",
    "             if isinstance(hp_sig_terms, (int, float)) and hp_sig_terms > 0:\n",
    "                  summary_text_lines.append(f\"- **H+P Reference (2D):** The baseline H+P ruleset generated a dynamic region significantly enriched ({hp_sig_terms} terms), with '{hp_top_term}' as a top term. This confirms that the combination of Harmonic and Pheromone can produce dynamically active regions associated with relevant biology.\")\n",
    "             else:\n",
    "                  summary_text_lines.append(f\"- **H+P Reference (2D):** The dynamic region from the baseline H+P run did not show significant enrichment ({hp_sig_terms} terms found). This suggests that while it generated dynamic activity, that activity was not spatially organized in a way that strongly highlights GO BP functions under the chosen parameters and dynamic region definition.\")\n",
    "        else:\n",
    "            summary_text_lines.append(\"- Interpretation Note: H+P Reference run enrichment data missing.\")\n",
    "\n",
    "        if 'H+4D-Bio (AIFM1)' in dynamic_biological_analysis_comparison_df.index:\n",
    "             bio_sig_terms = dynamic_biological_analysis_comparison_df.loc['H+4D-Bio (AIFM1)'].get('Significant Terms', 0)\n",
    "             bio_top_term = dynamic_biological_analysis_comparison_df.loc['H+4D-Bio (AIFM1)'].get('Top Term', 'N/A')\n",
    "             if isinstance(bio_sig_terms, (int, float)) and bio_sig_terms > 0:\n",
    "                  summary_text_lines.append(f\"- **H+4D-Bio (AIFM1):** The dynamically active region from the 4D Biological model on AIFM1 was also significantly enriched ({bio_sig_terms} terms), with '{bio_top_term}' as a top term. This demonstrates that incorporating biologically specific state variables and rules can produce dynamically relevant regions associated with biological functions.\")\n",
    "             else:\n",
    "                  summary_text_lines.append(f\"- **H+4D-Bio (AIFM1):** The dynamic region from the 4D Biological run did not show significant enrichment ({bio_sig_terms} terms found).\")\n",
    "        else:\n",
    "            summary_text_lines.append(\"- Interpretation Note: H+4D-Bio run enrichment data missing.\")\n",
    "\n",
    "        if 'P-Only (2D)' in dynamic_biological_analysis_comparison_df.index:\n",
    "             p_only_sig_terms = dynamic_biological_analysis_comparison_df.loc['P-Only (2D)'].get('Significant Terms', 0)\n",
    "             if isinstance(p_only_sig_terms, (int, float)):\n",
    "                  summary_text_lines.append(f\"- **Pheromone Only (2D):** This run, which collapsed to homogeneity, showed {p_only_sig_terms} significant terms. If this number is low or zero, it reinforces that homogeneity is not biologically relevant in this context.\")\n",
    "             else:\n",
    "                  summary_text_lines.append(\"- Interpretation Note: P-Only run enrichment data missing.\")\n",
    "\n",
    "        if 'H-Only (2D)' in dynamic_biological_analysis_comparison_df.index:\n",
    "             h_only_sig_terms = dynamic_biological_analysis_comparison_df.loc['H-Only (2D)'].get('Significant Terms', 0)\n",
    "             if isinstance(h_only_sig_terms, (int, float)):\n",
    "                  summary_text_lines.append(f\"- **Harmonic Only (2D):** This run generated strong oscillations but lacked Pheromone stabilization. It showed {h_only_sig_terms} significant terms. Comparing this to the H+P run highlights if Pheromone *enhances* or *focuses* the biological signal within the dynamic region.\")\n",
    "             else:\n",
    "                  summary_text_lines.append(\"- Interpretation Note: H-Only run enrichment data missing.\")\n",
    "\n",
    "        generic_ph_labels = [\"H+3D-PH (Coupled)\", \"H+5D-PH (Coupled)\", \"H+5D-PH (Decoupled)\"]\n",
    "        generic_ph_sig_terms = {}\n",
    "        for label in generic_ph_labels:\n",
    "             if label in dynamic_biological_analysis_comparison_df.index:\n",
    "                  sig_terms = dynamic_biological_analysis_comparison_df.loc[label].get('Significant Terms', 0)\n",
    "                  if isinstance(sig_terms, (int, float)):\n",
    "                       generic_ph_sig_terms[label] = sig_terms\n",
    "             else:\n",
    "                  generic_ph_sig_terms[label] = \"Data Missing\"\n",
    "\n",
    "        if generic_ph_sig_terms:\n",
    "             summary_text_lines.append(\"- **Generic Placeholder Runs:**\")\n",
    "             for label, sig_terms in generic_ph_sig_terms.items():\n",
    "                  if isinstance(sig_terms, (int, float)):\n",
    "                       summary_text_lines.append(f\"  - The Dynamic Regions from {label} showed {sig_terms} significant terms. If this number is low compared to H+P or H+4D-Bio, it suggests generic dimensions/dynamics are less effective at highlighting biologically relevant regions.\")\n",
    "                  else:\n",
    "                       summary_text_lines.append(f\"  - Data missing for {label}.\")\n",
    "\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"Conclusion: Comparing the number and nature of significant terms across runs allows us to infer which ruleset components are most effective at generating dynamically active regions that are biologically interpretable via GO enrichment.\")\n",
    "        summary_text_lines.append(\"\")\n",
    "        summary_text_lines.append(\"---\")\n",
    "\n",
    "        dynamic_biological_analysis_summary_markdown = \"\\n\".join(summary_text_lines)\n",
    "\n",
    "        print(\"✅ Dynamic biological analysis summary markdown generated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating dynamic biological analysis summary markdown: {e}\")\n",
    "        traceback.print_exc()\n",
    "        markdown_gen_error = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipping dynamic biological analysis summary markdown generation due to previous errors.\")\n",
    "\n",
    "# Store globally\n",
    "globals()['dynamic_biological_analysis_summary_markdown'] = dynamic_biological_analysis_summary_markdown\n",
    "\n",
    "print(\"\\nCell 8: Dynamic biological analysis summary markdown generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b94c1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 9: Save and Display Dynamic Biological Analysis Summary Markdown (2025-04-28 21:19:11) ---\n",
      "✅ Dynamic Biological Analysis Summary saved to: biological_analysis_results/Dynamic_Biological_Analysis/dynamic_biological_analysis_summary.md\n",
      "\n",
      "--- Displaying Dynamic Biological Analysis Summary ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Biological Relevance of Dynamic Regions Across Ruleset Ablations (TargetProtein Subgraph)\n",
       "\n",
       "## 1. Introduction\n",
       "This analysis investigates the biological relevance of the **dynamically active regions** identified in simulations of various Network Automaton rulesets applied to the AIFM1 subgraph. By performing functional enrichment analysis on the gene sets corresponding to these dynamic regions, we assess which ruleset components contribute to generating dynamically organized patterns that are associated with known biological functions.\n",
       "\n",
       "The dynamically active regions were identified using a consistent metric (time-averaged absolute state change over the final 20% window) and thresholded at the 80th percentile across all nodes.\n",
       "\n",
       "## 2. Comparative Enrichment Analysis Table\n",
       "The table below summarizes the functional enrichment results (GO Biological Process) for the dynamic region identified in each simulation run:\n",
       "\n",
       "*(Table generation failed: 'tabulate' library missing. Install it.)*\n",
       "\n",
       "_**Table Columns:**_\n",
       "- **Dynamic Region Size:** Number of nodes in the dynamic region for that run.\n",
       "- **Mapped Genes:** Number of unique gene symbols mapped from the dynamic region nodes.\n",
       "- **Significant Terms:** Number of GO BP terms found to be significantly enriched (Adj. P < 0.05).\n",
       "- **Top Term:** The GO BP term with the most significant adjusted P-value.\n",
       "- **Top Term Adj P:** The adjusted P-value for the Top Term.\n",
       "\n",
       "## 3. Interpretation of Biological Relevance\n",
       "Based on the comparative enrichment analysis:\n",
       "\n",
       "- **H+P Reference (2D):** The dynamic region from the baseline H+P run did not show significant enrichment (2612 terms found). This suggests that while it generated dynamic activity, that activity was not spatially organized in a way that strongly highlights GO BP functions under the chosen parameters and dynamic region definition.\n",
       "- **H+4D-Bio (AIFM1):** The dynamic region from the 4D Biological run did not show significant enrichment (2700 terms found).\n",
       "- Interpretation Note: P-Only run enrichment data missing.\n",
       "- Interpretation Note: H-Only run enrichment data missing.\n",
       "\n",
       "Conclusion: Comparing the number and nature of significant terms across runs allows us to infer which ruleset components are most effective at generating dynamically active regions that are biologically interpretable via GO enrichment.\n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- End of Display ---\n",
      "\n",
      "Cell 9: Save and Display Dynamic Biological Analysis Summary Markdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save and Display Dynamic Biological Analysis Summary Markdown\n",
    "# Description: Saves the generated markdown text to a file in the dynamic biological\n",
    "#              analysis results directory and prints it to the console.\n",
    "\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display, Markdown # For displaying markdown\n",
    "\n",
    "print(f\"\\n--- Cell 9: Save and Display Dynamic Biological Analysis Summary Markdown ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "save_display_error = False\n",
    "if 'dynamic_biological_analysis_summary_markdown' not in globals() or not dynamic_biological_analysis_summary_markdown:\n",
    "    print(\"❌ Cannot save/display: 'dynamic_biological_analysis_summary_markdown' missing or empty (Run Cell 8).\"); save_display_error = True\n",
    "if 'OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS' not in globals() or not OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS:\n",
    "    print(\"❌ Cannot save: OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS missing (Run Cell 1).\"); save_display_error = True\n",
    "elif not os.path.isdir(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS):\n",
    "     print(f\"❌ Cannot save: OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS directory not found: {OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS}. Check Cell 1.\"); save_display_error = True\n",
    "\n",
    "# --- Execute Save and Display ---\n",
    "if not save_display_error:\n",
    "    summary_markdown_path = os.path.join(OUTPUT_DIR_DYNAMIC_BIO_ANALYSIS, \"dynamic_biological_analysis_summary.md\") # Specific filename\n",
    "\n",
    "    try:\n",
    "        with open(summary_markdown_path, 'w') as f:\n",
    "            f.write(dynamic_biological_analysis_summary_markdown)\n",
    "        print(f\"✅ Dynamic Biological Analysis Summary saved to: {summary_markdown_path}\")\n",
    "\n",
    "        print(\"\\n--- Displaying Dynamic Biological Analysis Summary ---\")\n",
    "        # Use IPython.display.Markdown to render the markdown in the notebook output\n",
    "        display(Markdown(dynamic_biological_analysis_summary_markdown))\n",
    "        print(\"--- End of Display ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving or displaying summary markdown: {e}\")\n",
    "        traceback.print_exc()\n",
    "        save_display_error = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipping save/display due to previous errors.\")\n",
    "\n",
    "print(\"\\nCell 9: Save and Display Dynamic Biological Analysis Summary Markdown complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

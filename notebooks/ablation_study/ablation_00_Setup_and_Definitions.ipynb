{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac60942b",
   "metadata": {},
   "source": [
    "# Cell 0: Notebook Header & Documentation\n",
    "# Description: Provides context and instructions for this setup notebook.\n",
    "\n",
    "## Notebook Title: Ablation Study - Setup, Configuration & Data Generation\n",
    "\n",
    "### Purpose and Context\n",
    "\n",
    "*   **Goal:** To establish the common foundation for the AIFM1 Network Automaton ablation study. This includes importing libraries, setting up the environment, defining baseline configurations, preparing the AIFM1 network subgraph, and saving essential configuration and graph data outputs to files.\n",
    "*   **Contribution:** Generates the standardized input files (configuration, graph, node lists, layout, seed index) used by subsequent experimental notebooks (`ablation_01` onwards). Defines base directories and a MASTER_SEED.\n",
    "*   **Inputs:** Requires access to STRING database files (will download if not present).\n",
    "*   **Outputs:**\n",
    "    *   Creates base output directories (`simulation_results`, `biological_analysis_results`).\n",
    "    *   Creates a dedicated setup output directory (`simulation_results/Ablation_Setup_Files`).\n",
    "    *   Saves the baseline configuration (including `MASTER_SEED`) to `baseline_config.json`.\n",
    "    *   Saves the prepared NetworkX graph `G` to `graph_G.pkl`.\n",
    "    *   Saves the calculated layout `pos` to `graph_pos.pkl`.\n",
    "    *   Saves `node_list`, `node_to_int`, `int_to_node`, and `INITIAL_SEED_NODES_IDX` to respective `.pkl` files.\n",
    "    *   Saves device information (`device.json`).\n",
    "\n",
    "### How to Run\n",
    "\n",
    "*   **Prerequisites:** Python environment with necessary libraries installed (`pandas`, `networkx`, `numpy`, `requests`, `tqdm`, `torch`).\n",
    "*   **Configuration:** Check/set base directory paths (`DATA_ROOT_DIR`, `OUTPUT_DIR`, `ANALYSIS_DIR`), `MASTER_SEED`, `TARGET_NODE_ID`, and baseline parameters in Cell 2 if defaults need changing.\n",
    "*   **Execution:** Run all cells sequentially from top to bottom (Cell 1 through Cell 3). **This notebook MUST be run successfully before any `ablation_01` through `ablation_08` notebooks.**\n",
    "*   **Expected Runtime:** Variable depending on download/extraction time for STRING data (minutes), graph preparation and layout calculation (minutes). File saving is fast.\n",
    "\n",
    "### Expected Results & Analysis (within this notebook)\n",
    "\n",
    "*   This notebook performs setup, configuration, and graph preparation, saving outputs to files.\n",
    "*   Successful execution results in:\n",
    "    *   Confirmation messages for setup steps.\n",
    "    *   Logs detailing graph preparation.\n",
    "    *   Printout of AIFM1 subgraph properties.\n",
    "    *   Confirmation messages listing saved files in `simulation_results/Ablation_Setup_Files/`.\n",
    "*   **NO simulations are run, NO analysis is performed, and NO functions are defined for external use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad2fb7",
   "metadata": {},
   "source": [
    "Copyright 2025 Michael G. Young II, Emergenics Foundation\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe4509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cell 1: Initial Setup & Imports (2025-04-28 20:44:56) ---\n",
      "✅ CUDA available, using default GPU: cuda:0 (NVIDIA GeForce RTX 2060)\n",
      "Checked/created base directories.\n",
      "Setup files will be saved in: simulation_results/Ablation_Setup_Files\n",
      "  ✅ Saved device info to simulation_results/Ablation_Setup_Files/device_info.json\n",
      "\n",
      "Cell 1: Initial Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initial Setup, Imports, Device Check\n",
    "# Description: Basic imports, setup directories, checks for GPU availability, saves device info.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Only import plotting libraries if actually plotting *in this notebook*\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.cm as cm\n",
    "# import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "import requests\n",
    "import io\n",
    "import gzip\n",
    "import shutil\n",
    "import copy\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import traceback\n",
    "import random # Import random for seeding\n",
    "# Keep imports needed by graph prep functions defined later\n",
    "from tqdm.auto import tqdm\n",
    "import gc # For memory management\n",
    "\n",
    "# Ignore common warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "print(f\"--- Cell 1: Initial Setup & Imports ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Device Check ---\n",
    "dev_name = None # Initialize\n",
    "if torch.cuda.is_available():\n",
    "    # Check actual device assignment just in case\n",
    "    try:\n",
    "        # Try to get the current device index, default to 0 if error\n",
    "        current_dev_index = torch.cuda.current_device()\n",
    "        device = torch.device(f'cuda:{current_dev_index}')\n",
    "        dev_name = torch.cuda.get_device_name(current_dev_index)\n",
    "        print(f\"✅ CUDA available, using default GPU: {device} ({dev_name})\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CUDA available, but error getting current device: {e}. Falling back to cuda:0.\")\n",
    "        device = torch.device('cuda:0') # Fallback\n",
    "        try:\n",
    "            dev_name = torch.cuda.get_device_name(0)\n",
    "        except Exception:\n",
    "             dev_name = \"CUDA Device (Unknown Name)\"\n",
    "        print(f\"   Using fallback: {device} ({dev_name})\")\n",
    "    device_type = 'cuda'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"⚠️ CUDA not available, using CPU.\")\n",
    "    device_type = 'cpu'\n",
    "    dev_name = 'CPU'\n",
    "\n",
    "# --- Base Directories ---\n",
    "DATA_ROOT_DIR = \"/tmp/cakg_data\"\n",
    "OUTPUT_DIR = \"simulation_results\"\n",
    "ANALYSIS_DIR = \"biological_analysis_results\"\n",
    "SETUP_OUTPUT_SUBDIR = \"Ablation_Setup_Files\" # Specific folder for setup outputs\n",
    "SETUP_OUTPUT_DIR = os.path.join(OUTPUT_DIR, SETUP_OUTPUT_SUBDIR)\n",
    "\n",
    "# Create all directories\n",
    "os.makedirs(DATA_ROOT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "os.makedirs(SETUP_OUTPUT_DIR, exist_ok=True) # Create the specific setup dir\n",
    "print(f\"Checked/created base directories.\")\n",
    "print(f\"Setup files will be saved in: {SETUP_OUTPUT_DIR}\")\n",
    "\n",
    "# --- Save Device Info ---\n",
    "# Use standard library types for JSON compatibility\n",
    "device_info = {'device_type': device_type, 'device_name': dev_name, 'torch_device_str': str(device)}\n",
    "device_info_path = os.path.join(SETUP_OUTPUT_DIR, \"device_info.json\")\n",
    "try:\n",
    "    with open(device_info_path, 'w') as f:\n",
    "        json.dump(device_info, f, indent=4)\n",
    "    print(f\"  ✅ Saved device info to {device_info_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠️ Warning: Could not save device info: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 1: Initial Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872180af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 2: Baseline Configuration Definition & Saving ---\n",
      "MASTER_SEED set to: 42\n",
      "Baseline State Dim: 2\n",
      "Baseline Seed Value: [1.0, 0.0]\n",
      "Baseline Default State: [0.0, 0.0]\n",
      "\n",
      "--- Baseline Rule Parameters (Initial: 2D H+P Active) ---\n",
      "{\n",
      "  \"activation_threshold\": 0.5,\n",
      "  \"activation_increase_rate\": 0.15,\n",
      "  \"activation_decay_rate\": 0.05,\n",
      "  \"inhibition_threshold\": 0.5,\n",
      "  \"inhibition_increase_rate\": 0.1,\n",
      "  \"inhibition_decay_rate\": 0.1,\n",
      "  \"inhibition_feedback_threshold\": 0.6,\n",
      "  \"inhibition_feedback_strength\": 0.3,\n",
      "  \"diffusion_factor\": 0.05,\n",
      "  \"noise_level\": 0.001,\n",
      "  \"harmonic_factor\": 0.05,\n",
      "  \"pheromone_increase_rate\": 0.02,\n",
      "  \"pheromone_multiplicative_decay_rate\": 0.99,\n",
      "  \"w_decay_rate\": 0.0,\n",
      "  \"x_decay_rate\": 0.0,\n",
      "  \"y_decay_rate\": 0.0,\n",
      "  \"placeholder_decay_rate\": 0.0,\n",
      "  \"refractory_threshold\": 1.1,\n",
      "  \"additional_decay_factor\": 0.0,\n",
      "  \"use_dynamic_weights\": false,\n",
      "  \"potentiation_threshold\": 1.1,\n",
      "  \"potentiation_increase_rate\": 0.0,\n",
      "  \"potentiation_decay_rate\": 1.0,\n",
      "  \"max_potentiation\": 0.0,\n",
      "  \"use_confidence_weight\": true\n",
      "}\n",
      "\n",
      "✅ Saved baseline configuration (including MASTER_SEED) to: simulation_results/Ablation_Setup_Files/baseline_config.json\n",
      "\n",
      "Cell 2: Baseline configuration defined and saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Baseline Configuration Definition & Saving (Includes Master Seed)\n",
    "# Description: Defines baseline configuration, rule parameters (typically 2D H+P), and MASTER_SEED.\n",
    "#              Saves the complete configuration dictionary to a JSON file in the setup directory.\n",
    "#              This configuration will be loaded by subsequent notebooks.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import copy\n",
    "import random # Import random for seeding\n",
    "\n",
    "print(f\"\\n--- Cell 2: Baseline Configuration Definition & Saving ---\")\n",
    "\n",
    "# Retrieve setup output path defined in Cell 1\n",
    "SETUP_OUTPUT_DIR = globals().get(\"SETUP_OUTPUT_DIR\")\n",
    "if SETUP_OUTPUT_DIR is None or not os.path.isdir(SETUP_OUTPUT_DIR):\n",
    "    # Attempt to recreate if missing, maybe from globals set in Cell 1 if run standalone\n",
    "    output_base = globals().get(\"OUTPUT_DIR\", \"simulation_results\")\n",
    "    setup_subdir = \"Ablation_Setup_Files\"\n",
    "    SETUP_OUTPUT_DIR = os.path.join(output_base, setup_subdir)\n",
    "    print(f\"Warning: SETUP_OUTPUT_DIR not found globally, attempting default: {SETUP_OUTPUT_DIR}\")\n",
    "    os.makedirs(SETUP_OUTPUT_DIR, exist_ok=True) # Create if doesn't exist\n",
    "\n",
    "# --- MASTER SEED for Reproducibility ---\n",
    "MASTER_SEED = 42\n",
    "print(f\"MASTER_SEED set to: {MASTER_SEED}\")\n",
    "# Apply seed globally in this setup notebook for consistency in graph/layout\n",
    "np.random.seed(MASTER_SEED)\n",
    "random.seed(MASTER_SEED)\n",
    "\n",
    "# --- Experiment Info ---\n",
    "DATASET_NAME = \"STRING\"\n",
    "# Define a base name that later notebooks can modify/append\n",
    "BASE_EXPERIMENT_NAME = 'string_ca_subgraph_AIFM1_CORRECTED' # Base name for study\n",
    "\n",
    "# --- Subgraph Selection (TARGET AIFM1) ---\n",
    "TARGET_NODE_ID = '9606.ENSP00000287295' # AIFM1 ID\n",
    "SUBGRAPH_RADIUS = 2\n",
    "\n",
    "# --- Baseline State Parameters (2D H+P) ---\n",
    "STATE_DIM = 2 # Start with 2D as baseline default\n",
    "ACT_IDX = 0; INH_IDX = 1\n",
    "PLACEHOLDER_W_IDX = 2; PLACEHOLDER_X_IDX = 3; PLACEHOLDER_Y_IDX = 4 # Define indices even if dim=2\n",
    "\n",
    "INIT_MODE = 'seeds'\n",
    "SEED_ACTIVATION_VALUE = [1.0, 0.0] # Use list for JSON compatibility\n",
    "DEFAULT_INACTIVE_STATE = [0.0, 0.0] # Use list for JSON compatibility\n",
    "\n",
    "# --- Simulation Parameters ---\n",
    "MAX_SIMULATION_STEPS = 500\n",
    "CONVERGENCE_THRESHOLD = 0.0001\n",
    "\n",
    "# --- Visualization & Output Parameters ---\n",
    "NODES_TO_PLOT_COUNT = 10\n",
    "SNAPSHOT_STEPS = [0, 50, 100, 250, 499] # Baseline snapshot steps (adjust end for 0-based indexing)\n",
    "\n",
    "# --- Baseline Rule Parameters (2D H+P Configuration) ---\n",
    "# Contains ALL potential parameters used across different ablation runs,\n",
    "# with defaults set for the baseline 2D H+P run.\n",
    "rule_params = {\n",
    "    'activation_threshold': 0.5, 'activation_increase_rate': 0.15, 'activation_decay_rate': 0.05,\n",
    "    'inhibition_threshold': 0.5, 'inhibition_increase_rate': 0.1, 'inhibition_decay_rate': 0.1,\n",
    "    'inhibition_feedback_threshold': 0.6, 'inhibition_feedback_strength': 0.3,\n",
    "    'diffusion_factor': 0.05,\n",
    "    'noise_level': 0.001,\n",
    "    'harmonic_factor': 0.05,             # H Active in baseline\n",
    "    'pheromone_increase_rate': 0.02,     # P Active in baseline\n",
    "    'pheromone_multiplicative_decay_rate': 0.99,\n",
    "    # Placeholder decay keys (inactive in baseline)\n",
    "    'w_decay_rate': 0.0, 'x_decay_rate': 0.0, 'y_decay_rate': 0.0, 'placeholder_decay_rate': 0.0,\n",
    "    # Refractory/Dynamic Weights keys (inactive in baseline)\n",
    "    'refractory_threshold': 1.1, 'additional_decay_factor': 0.0, 'use_dynamic_weights': False,\n",
    "    'potentiation_threshold': 1.1, 'potentiation_increase_rate': 0.0, 'potentiation_decay_rate': 1.0, 'max_potentiation': 0.0,\n",
    "    # Weight usage flag\n",
    "    'use_confidence_weight': True,\n",
    "}\n",
    "\n",
    "# --- Combine All Config into One Dictionary ---\n",
    "# This dictionary is saved to file for other notebooks to load\n",
    "baseline_config = {\n",
    "    # Reproducibility\n",
    "    'MASTER_SEED': MASTER_SEED,\n",
    "    # Experiment Info\n",
    "    'DATASET_NAME': DATASET_NAME,\n",
    "    'EXPERIMENT_NAME': BASE_EXPERIMENT_NAME, # Base name saved\n",
    "    'TARGET_NODE_ID': TARGET_NODE_ID,\n",
    "    'SUBGRAPH_RADIUS': SUBGRAPH_RADIUS,\n",
    "    # State Info\n",
    "    'STATE_DIM': STATE_DIM, # Save the baseline state dim\n",
    "    'ACT_IDX': ACT_IDX, 'INH_IDX': INH_IDX,\n",
    "    'PLACEHOLDER_W_IDX': PLACEHOLDER_W_IDX, 'PLACEHOLDER_X_IDX': PLACEHOLDER_X_IDX, 'PLACEHOLDER_Y_IDX': PLACEHOLDER_Y_IDX,\n",
    "    'INIT_MODE': INIT_MODE,\n",
    "    'SEED_ACTIVATION_VALUE': SEED_ACTIVATION_VALUE,\n",
    "    'DEFAULT_INACTIVE_STATE': DEFAULT_INACTIVE_STATE,\n",
    "    # Simulation Params\n",
    "    'MAX_SIMULATION_STEPS': MAX_SIMULATION_STEPS,\n",
    "    'CONVERGENCE_THRESHOLD': CONVERGENCE_THRESHOLD,\n",
    "    # Visualization Params\n",
    "    'NODES_TO_PLOT_COUNT': NODES_TO_PLOT_COUNT,\n",
    "    'SNAPSHOT_STEPS': SNAPSHOT_STEPS,\n",
    "    # Rule Params (Contains ALL possible keys, values set for baseline)\n",
    "    'rule_params': rule_params,\n",
    "    # Base Dirs (useful for context when loading)\n",
    "    'DATA_ROOT_DIR': globals().get('DATA_ROOT_DIR', '/tmp/cakg_data'),\n",
    "    'OUTPUT_DIR': globals().get('OUTPUT_DIR', 'simulation_results'),\n",
    "    'ANALYSIS_DIR': globals().get('ANALYSIS_DIR', 'biological_analysis_results'),\n",
    "    'SETUP_OUTPUT_DIR': SETUP_OUTPUT_DIR\n",
    "}\n",
    "\n",
    "print(f\"Baseline State Dim: {baseline_config['STATE_DIM']}\")\n",
    "print(f\"Baseline Seed Value: {baseline_config['SEED_ACTIVATION_VALUE']}\")\n",
    "print(f\"Baseline Default State: {baseline_config['DEFAULT_INACTIVE_STATE']}\")\n",
    "print(\"\\n--- Baseline Rule Parameters (Initial: 2D H+P Active) ---\")\n",
    "print(json.dumps(baseline_config['rule_params'], indent=2))\n",
    "\n",
    "# --- Save Configuration to File ---\n",
    "config_save_path = os.path.join(SETUP_OUTPUT_DIR, \"baseline_config.json\")\n",
    "try:\n",
    "    with open(config_save_path, 'w') as f:\n",
    "        # Use default=str to handle potential numpy types if they crept in\n",
    "        json.dump(baseline_config, f, indent=4, default=str)\n",
    "    print(f\"\\n✅ Saved baseline configuration (including MASTER_SEED) to: {config_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error saving baseline configuration: {e}\")\n",
    "    traceback.print_exc(limit=1)\n",
    "\n",
    "print(\"\\nCell 2: Baseline configuration defined and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44a25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 3: Graph Preparation and Saving (AIFM1 Subgraph) ---\n",
      "  Loaded config from simulation_results/Ablation_Setup_Files/baseline_config.json\n",
      "  Target Node: 9606.ENSP00000287295, Radius: 2, Score Threshold: 0.6\n",
      "File already exists: /tmp/cakg_data/STRING/9606.protein.links.full.v12.0.txt.gz\n",
      "Extracted file /tmp/cakg_data/STRING/9606.protein.links.full.v12.0.txt up-to-date. Skipping.\n",
      "Using STRING data file: /tmp/cakg_data/STRING/9606.protein.links.full.v12.0.txt\n",
      "Loading STRING data...\n",
      "Loaded 9310247 raw interactions.\n",
      "  Dropped 1 rows with non-numeric score.\n",
      "  Score column cleaned and converted to integer.\n",
      "✅ Target '9606.ENSP00000287295' present in raw data.\n",
      "Filtered edges by score >= 600. Kept 490325 / 9310246 interactions.\n",
      "Building NetworkX graph (G_full)...\n",
      "Added 17674 nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c6b03d614243a4ae7af6fa679d6c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding Edges:   0%|          | 0/490325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built full filtered graph: 17674 nodes, 322463 edges.\n",
      "Extracting ego graph (radius 2) for '9606.ENSP00000287295'...\n",
      "Final graph 'G' created: 2334 nodes, 79930 edges.\n",
      "Building node list and mappings...\n",
      "Node list length: 2334\n",
      "Target '9606.ENSP00000287295' mapped to index: [556]\n",
      "Calculating graph layout (Spring)...\n",
      "  Layout params: k=0.017, iterations=75\n",
      "  Layout finished in 33.88 sec.\n",
      "\n",
      "--- Graph Preparation Steps Completed ---\n",
      "\n",
      "--- Saving Graph Data to Files in simulation_results/Ablation_Setup_Files ---\n",
      "  ✅ Saved graph_G.pkl\n",
      "  ✅ Saved graph_pos.pkl\n",
      "  ✅ Saved node_list.pkl\n",
      "  ✅ Saved node_to_int.pkl\n",
      "  ✅ Saved int_to_node.pkl\n",
      "  ✅ Saved initial_seed_nodes_idx.pkl\n",
      "--- Graph data saving complete. ---\n",
      "\n",
      "--- Verifying Saved Files ---\n",
      "  ✅ Essential saved files verified.\n",
      "\n",
      "Cell 3: Graph Preparation and Saving execution complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Graph Preparation and Saving\n",
    "# Description: Downloads/extracts STRING data, filters by score, builds NetworkX graph G,\n",
    "#              extracts AIFM1 ego subgraph, calculates layout 'pos', generates node lists/mappings,\n",
    "#              and saves all graph-related objects to files in the setup directory.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import traceback\n",
    "import time\n",
    "import pickle # Needed for saving graph objects\n",
    "import json # Needed for loading config path\n",
    "import gc # Import garbage collector\n",
    "\n",
    "print(\"\\n--- Cell 3: Graph Preparation and Saving (AIFM1 Subgraph) ---\")\n",
    "\n",
    "# --- Load Config for Paths and Parameters ---\n",
    "config = {}\n",
    "graph_prep_error = False\n",
    "local_G = None; local_node_list = []; local_node_to_int = {}; local_int_to_node = {}; local_pos = None; local_INITIAL_SEED_NODES_IDX = []\n",
    "SETUP_OUTPUT_DIR_save = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Load config saved by Cell 2\n",
    "    SETUP_OUTPUT_DIR_load = os.path.join(\"simulation_results\", \"Ablation_Setup_Files\")\n",
    "    if not os.path.isdir(SETUP_OUTPUT_DIR_load): raise NotADirectoryError(f\"Setup directory not found: {SETUP_OUTPUT_DIR_load}. Run Cell 1 & 2 first.\")\n",
    "    config_path_load = os.path.join(SETUP_OUTPUT_DIR_load, \"baseline_config.json\")\n",
    "    if not os.path.exists(config_path_load): raise FileNotFoundError(f\"Baseline config file not found: {config_path_load}\")\n",
    "    with open(config_path_load, 'r') as f: config = json.load(f)\n",
    "    print(f\"  Loaded config from {config_path_load}\")\n",
    "\n",
    "    # Extract necessary parameters\n",
    "    DATA_ROOT_DIR_local = config.get('DATA_ROOT_DIR')\n",
    "    DATASET_NAME_local = config.get('DATASET_NAME', 'STRING')\n",
    "    TARGET_NODE_ID_local = config.get('TARGET_NODE_ID')\n",
    "    SUBGRAPH_RADIUS_local = config.get('SUBGRAPH_RADIUS')\n",
    "    SCORE_THRESHOLD_local = 0.6 # Fixed threshold for consistency\n",
    "    INIT_MODE_local = config.get('INIT_MODE')\n",
    "    SETUP_OUTPUT_DIR_save = config.get('SETUP_OUTPUT_DIR') # Use path from loaded config\n",
    "    MASTER_SEED = config.get('MASTER_SEED', 42)\n",
    "\n",
    "    if None in [DATA_ROOT_DIR_local, TARGET_NODE_ID_local, SUBGRAPH_RADIUS_local, INIT_MODE_local, SETUP_OUTPUT_DIR_save]:\n",
    "        raise ValueError(\"Essential parameters missing from loaded config.\")\n",
    "    print(f\"  Target Node: {TARGET_NODE_ID_local}, Radius: {SUBGRAPH_RADIUS_local}, Score Threshold: {SCORE_THRESHOLD_local}\")\n",
    "\n",
    "except Exception as e_conf:\n",
    "    print(f\"❌ Error loading config for graph prep: {e_conf}\")\n",
    "    graph_prep_error = True\n",
    "\n",
    "# --- Helper Functions (Define locally for self-containment) ---\n",
    "def download_file(url, dest_path, desc=None):\n",
    "    \"\"\"Downloads a file with progress, robust error handling.\"\"\"\n",
    "    if os.path.exists(dest_path):\n",
    "        print(f\"File already exists: {dest_path}\")\n",
    "        return dest_path\n",
    "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "    effective_desc = desc or os.path.basename(dest_path)\n",
    "    print(f\"Downloading {url} to {dest_path}...\")\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, stream=True, timeout=300, headers=headers) # Increased timeout\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(dest_path, 'wb') as f, tqdm(\n",
    "            desc=effective_desc, total=total_size, unit='iB',\n",
    "            unit_scale=True, unit_divisor=1024, miniters=1, leave=False\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(chunk_size=1024*1024): # Larger chunk size\n",
    "                if chunk:\n",
    "                    size = f.write(chunk)\n",
    "                    bar.update(size)\n",
    "        actual_size = os.path.getsize(dest_path)\n",
    "        if total_size != 0 and actual_size < total_size :\n",
    "            warnings.warn(f\"Size mismatch (may be OK): Expected {total_size}, got {actual_size}\")\n",
    "        print(f\"Download completed: {dest_path}\")\n",
    "        return dest_path\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        if os.path.exists(dest_path):\n",
    "             try: os.remove(dest_path); print(f\"Removed partial download: {dest_path}\")\n",
    "             except OSError as rm_err: print(f\"Error removing partial download {dest_path}: {rm_err}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected download error: {e}\")\n",
    "        if os.path.exists(dest_path):\n",
    "             try: os.remove(dest_path); print(f\"Removed partial download: {dest_path}\")\n",
    "             except OSError as rm_err: print(f\"Error removing partial download {dest_path}: {rm_err}\")\n",
    "        raise\n",
    "\n",
    "def extract_file(src_path, dest_dir, desc=None):\n",
    "    \"\"\"Extracts .gz files with progress and robust checks.\"\"\"\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"Source not found: {src_path}\")\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    effective_desc = desc or f\"Extracting {os.path.basename(src_path)}\"\n",
    "    extracted_path = None\n",
    "    try:\n",
    "        if src_path.endswith('.gz') and not src_path.endswith('.tar.gz'):\n",
    "            dest_file = os.path.join(dest_dir, os.path.basename(src_path)[:-3])\n",
    "            if os.path.exists(dest_file) and os.path.getmtime(dest_file) >= os.path.getmtime(src_path):\n",
    "                print(f\"Extracted file {dest_file} up-to-date. Skipping.\")\n",
    "                return dest_file\n",
    "            print(f\"Extracting {src_path} to {dest_file}...\")\n",
    "            src_file_size = os.path.getsize(src_path)\n",
    "            with gzip.open(src_path, 'rb') as f_in, open(dest_file, 'wb') as f_out, tqdm(\n",
    "                total=src_file_size, unit='B', unit_scale=True, desc=effective_desc, leave=False\n",
    "            ) as pbar:\n",
    "                chunk_size = 1024 * 1024 # 1MB chunks\n",
    "                while True:\n",
    "                    chunk = f_in.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    bytes_written = f_out.write(chunk)\n",
    "                    # Attempt to update progress based on compressed file position\n",
    "                    try:\n",
    "                        new_pos = f_in.tell()\n",
    "                        update_amount = new_pos - pbar.n\n",
    "                        pbar.update(update_amount if update_amount > 0 else len(chunk))\n",
    "                    except Exception:\n",
    "                        pbar.update(len(chunk)) # Fallback to bytes written\n",
    "            extracted_path = dest_file\n",
    "        else:\n",
    "            print(f\"Unsupported format: {src_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "\n",
    "    if extracted_path and os.path.exists(extracted_path):\n",
    "        return extracted_path\n",
    "    else:\n",
    "        warnings.warn(f\"Extraction path '{extracted_path}' invalid or non-existent.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Graph Preparation Logic ---\n",
    "if not graph_prep_error:\n",
    "    try:\n",
    "        # Apply MASTER_SEED for layout reproducibility later\n",
    "        layout_seed = config.get('MASTER_SEED', 42)\n",
    "        random.seed(layout_seed)\n",
    "        np.random.seed(layout_seed)\n",
    "\n",
    "        # --- Download/Extract ---\n",
    "        string_species_id = TARGET_NODE_ID_local.split('.')[0]; string_version = \"v12.0\"; edge_url = f\"https://stringdb-downloads.org/download/protein.links.full.{string_version}/{string_species_id}.protein.links.full.{string_version}.txt.gz\"; dataset_root = os.path.join(DATA_ROOT_DIR_local, DATASET_NAME_local)\n",
    "        gz_filename = os.path.basename(edge_url); gz_file_path = os.path.join(dataset_root, gz_filename); extracted_filename = f\"{string_species_id}.protein.links.full.{string_version}.txt\"; edge_file_path = os.path.join(dataset_root, extracted_filename)\n",
    "        os.makedirs(dataset_root, exist_ok=True)\n",
    "        download_file(edge_url, gz_file_path, desc=\"STRING Links Archive\")\n",
    "        extracted_path_check = extract_file(gz_file_path, dataset_root, desc=\"Extracting STRING Links\")\n",
    "        if not extracted_path_check or not os.path.exists(edge_file_path): raise FileNotFoundError(f\"Required STRING file not found: {edge_file_path}\")\n",
    "        print(f\"Using STRING data file: {edge_file_path}\")\n",
    "\n",
    "        # --- Load Data & Clean Score ---\n",
    "        print(\"Loading STRING data...\")\n",
    "        required_cols = ['protein1', 'protein2', 'combined_score']; col_dtypes_load = {'protein1': str, 'protein2': str, 'combined_score': float}\n",
    "        edges_df = pd.read_csv(edge_file_path, sep=' ', usecols=required_cols, dtype=col_dtypes_load, low_memory=False)\n",
    "        print(f\"Loaded {len(edges_df)} raw interactions.\")\n",
    "        if edges_df.empty: raise ValueError(\"Loaded empty DataFrame.\")\n",
    "        initial_rows = len(edges_df); edges_df.dropna(subset=['combined_score'], inplace=True); rows_after_dropna = len(edges_df)\n",
    "        if rows_after_dropna < initial_rows: print(f\"  Dropped {initial_rows - rows_after_dropna} rows with non-numeric score.\")\n",
    "        edges_df['combined_score'] = edges_df['combined_score'].astype(int); print(\"  Score column cleaned and converted to integer.\")\n",
    "\n",
    "        # --- Pre-Filter Check ---\n",
    "        target_node_str = str(TARGET_NODE_ID_local)\n",
    "        if not (edges_df['protein1'].eq(target_node_str).any() or edges_df['protein2'].eq(target_node_str).any()): warnings.warn(f\"🚨 Target '{target_node_str}' NOT FOUND in raw data!\")\n",
    "        else: print(f\"✅ Target '{target_node_str}' present in raw data.\")\n",
    "\n",
    "        # --- Filter by Score ---\n",
    "        score_int_threshold = int(SCORE_THRESHOLD_local * 1000)\n",
    "        initial_count = len(edges_df)\n",
    "        edges_df = edges_df[edges_df['combined_score'] >= score_int_threshold].copy() # Apply filter after conversion\n",
    "        print(f\"Filtered edges by score >= {score_int_threshold}. Kept {len(edges_df)} / {initial_count} interactions.\")\n",
    "        if edges_df.empty: raise ValueError(f\"No interactions left after filtering at score {score_int_threshold}.\")\n",
    "\n",
    "        # --- Build Full Graph ---\n",
    "        print(\"Building NetworkX graph (G_full)...\")\n",
    "        G_full = nx.Graph()\n",
    "        unique_nodes = pd.unique(edges_df[['protein1', 'protein2']].values.ravel('K')); G_full.add_nodes_from(unique_nodes); print(f\"Added {G_full.number_of_nodes()} nodes.\")\n",
    "        skipped_self_loops = 0\n",
    "        for _, row in tqdm(edges_df.iterrows(), total=len(edges_df), desc=\"Adding Edges\"):\n",
    "            u, v, score = str(row['protein1']), str(row['protein2']), row['combined_score']\n",
    "            if u == v: skipped_self_loops += 1; continue\n",
    "            # Add edge only if it doesn't exist to avoid duplicate checks/warnings\n",
    "            if not G_full.has_edge(u,v):\n",
    "                 G_full.add_edge(u, v, weight=float(score / 1000.0))\n",
    "        if skipped_self_loops > 0: print(f\"Skipped {skipped_self_loops} self-loops.\")\n",
    "        print(f\"Built full filtered graph: {G_full.number_of_nodes()} nodes, {G_full.number_of_edges()} edges.\")\n",
    "        if target_node_str not in G_full: warnings.warn(f\"🚨 Target '{target_node_str}' NOT FOUND in filtered graph nodes!\")\n",
    "        del edges_df; gc.collect() # Free memory\n",
    "\n",
    "        # --- Extract Ego Graph ---\n",
    "        print(f\"Extracting ego graph (radius {SUBGRAPH_RADIUS_local}) for '{target_node_str}'...\")\n",
    "        temp_G = None\n",
    "        if target_node_str in G_full:\n",
    "            try:\n",
    "                temp_G = nx.ego_graph(G_full, target_node_str, radius=SUBGRAPH_RADIUS_local, undirected=True, center=True)\n",
    "            except Exception as ego_err:\n",
    "                print(f\"Error extracting ego graph: {ego_err}. Using full.\")\n",
    "                temp_G = G_full\n",
    "        else:\n",
    "             print(\"Target not found in filtered graph. Using full.\")\n",
    "             temp_G = G_full\n",
    "        del G_full; gc.collect() # Cleanup memory\n",
    "        if temp_G is None or temp_G.number_of_nodes() == 0:\n",
    "            raise ValueError(\"Final graph 'G' is None or empty.\")\n",
    "        local_G = temp_G # Assign to local variable for saving\n",
    "        print(f\"Final graph 'G' created: {local_G.number_of_nodes()} nodes, {local_G.number_of_edges()} edges.\")\n",
    "\n",
    "        # --- Build Mappings ---\n",
    "        print(\"Building node list and mappings...\")\n",
    "        local_node_list = sorted([str(n) for n in local_G.nodes()])\n",
    "        local_node_to_int = {node_id: i for i, node_id in enumerate(local_node_list)}\n",
    "        local_int_to_node = {i: node_id for node_id, i in local_node_to_int.items()}\n",
    "        print(f\"Node list length: {len(local_node_list)}\")\n",
    "\n",
    "        # --- Set Seed Index ---\n",
    "        local_INITIAL_SEED_NODES_IDX = []\n",
    "        if INIT_MODE_local == 'seeds' and target_node_str in local_node_to_int:\n",
    "            seed_idx = local_node_to_int[target_node_str]\n",
    "            local_INITIAL_SEED_NODES_IDX = [seed_idx] # Store as list\n",
    "            print(f\"Target '{target_node_str}' mapped to index: {local_INITIAL_SEED_NODES_IDX}\")\n",
    "        elif INIT_MODE_local == 'seeds':\n",
    "             warnings.warn(f\"Target '{target_node_str}' not in final map! Cannot set seed.\")\n",
    "\n",
    "        # --- Calculate Layout ---\n",
    "        print(\"Calculating graph layout (Spring)...\")\n",
    "        local_pos = None; layout_start_time = time.time()\n",
    "        try:\n",
    "            layout_seed = config.get('MASTER_SEED', 42)\n",
    "            random.seed(layout_seed); np.random.seed(layout_seed) # Re-seed just before layout\n",
    "            node_count = local_G.number_of_nodes(); k_val = 0.8 / np.sqrt(max(1, node_count)); iterations_val = 75\n",
    "            print(f\"  Layout params: k={k_val:.3f}, iterations={iterations_val}\")\n",
    "            local_pos = nx.spring_layout(local_G, k=k_val, iterations=iterations_val, seed=layout_seed)\n",
    "            layout_duration = time.time() - layout_start_time; print(f\"  Layout finished in {layout_duration:.2f} sec.\")\n",
    "            if len(local_pos) != node_count :\n",
    "                 warnings.warn(f\"Layout node count mismatch ({len(local_pos)} vs {node_count})\")\n",
    "        except Exception as layout_err:\n",
    "             print(f\"Layout error: {layout_err}. Setting pos=None.\"); local_pos = None\n",
    "\n",
    "        print(\"\\n--- Graph Preparation Steps Completed ---\")\n",
    "\n",
    "        # --- Save Graph Objects to Files ---\n",
    "        print(f\"\\n--- Saving Graph Data to Files in {SETUP_OUTPUT_DIR_save} ---\")\n",
    "        save_graph_error = False\n",
    "        objects_to_save = {\n",
    "            \"graph_G.pkl\": local_G,\n",
    "            \"graph_pos.pkl\": local_pos,\n",
    "            \"node_list.pkl\": local_node_list,\n",
    "            \"node_to_int.pkl\": local_node_to_int,\n",
    "            \"int_to_node.pkl\": local_int_to_node,\n",
    "            \"initial_seed_nodes_idx.pkl\": local_INITIAL_SEED_NODES_IDX # Save the potentially empty list\n",
    "        }\n",
    "        for filename, obj in objects_to_save.items():\n",
    "             filepath = os.path.join(SETUP_OUTPUT_DIR_save, filename)\n",
    "             # Save even if obj is None (like pos) or empty list\n",
    "             if obj is not None or isinstance(obj, (dict, list)) or filename == \"graph_pos.pkl\":\n",
    "                 try:\n",
    "                      with open(filepath, 'wb') as f:\n",
    "                           pickle.dump(obj, f)\n",
    "                      print(f\"  ✅ Saved {filename}\")\n",
    "                 except Exception as e:\n",
    "                      print(f\"  ❌ Error saving {filename}: {e}\"); save_graph_error = True\n",
    "             else:\n",
    "                  print(f\"  ⚠️ Skipping {filename} (object is None and not pos/list/dict).\")\n",
    "\n",
    "        if save_graph_error:\n",
    "            warnings.warn(\"Errors occurred during graph object saving.\")\n",
    "        else:\n",
    "            print(\"--- Graph data saving complete. ---\")\n",
    "\n",
    "    except Exception as prep_error:\n",
    "        print(f\"\\n❌❌❌ ERROR during graph preparation or saving: {prep_error} ❌❌❌\")\n",
    "        traceback.print_exc()\n",
    "        graph_prep_error = True\n",
    "\n",
    "else: # graph_prep_error was True from config loading\n",
    "    print(\"Skipping graph preparation due to configuration loading errors.\")\n",
    "\n",
    "# --- Final Sanity Check ---\n",
    "if not graph_prep_error:\n",
    "     print(\"\\n--- Verifying Saved Files ---\")\n",
    "     files_verified = True\n",
    "     try:\n",
    "         # Verify essential files were saved\n",
    "         essential_files = [\"graph_G.pkl\", \"node_list.pkl\", \"node_to_int.pkl\", \"int_to_node.pkl\", \"initial_seed_nodes_idx.pkl\"]\n",
    "         for fname in essential_files:\n",
    "             fpath = os.path.join(SETUP_OUTPUT_DIR_save, fname)\n",
    "             if not os.path.exists(fpath):\n",
    "                 print(f\"  ❌ Verification Error: Essential file '{fname}' not found.\")\n",
    "                 files_verified = False\n",
    "             else:\n",
    "                 # Optionally load and check content basic properties\n",
    "                 with open(fpath, 'rb') as f_check: obj_check = pickle.load(f_check)\n",
    "                 if fname == \"graph_G.pkl\" and (not isinstance(obj_check, nx.Graph) or obj_check.number_of_nodes() == 0):\n",
    "                     print(f\"  ❌ Verification Error: Loaded graph from '{fname}' is invalid or empty.\")\n",
    "                     files_verified = False\n",
    "                 elif fname == \"node_list.pkl\" and (not isinstance(obj_check, list)):\n",
    "                      print(f\"  ❌ Verification Error: Loaded node_list from '{fname}' is not a list.\")\n",
    "                      files_verified = False\n",
    "                 # Add more checks if needed...\n",
    "         if files_verified: print(\"  ✅ Essential saved files verified.\")\n",
    "\n",
    "     except Exception as e_verify:\n",
    "         print(f\"  Error verifying saved files: {e_verify}\")\n",
    "         files_verified = False\n",
    "     if not files_verified: print(\"  ⚠️ Verification failed. Check errors above.\")\n",
    "else:\n",
    "     print(\"\\n--- Graph Preparation Failed - Cannot Verify Outputs ---\")\n",
    "\n",
    "print(\"\\nCell 3: Graph Preparation and Saving execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3bb02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cell 3.1: Calculate and Save Static Baselines (Degree and RWR) (2025-04-28 20:46:19) ---\n",
      "Static baseline node lists will be saved to: simulation_results/baseline_nodes.txt\n",
      "Will calculate and save top 100 nodes for each baseline.\n",
      "\n",
      "Calculating static baseline node lists...\n",
      "  Calculating Top Degree nodes...\n",
      "  ✅ Calculated Top 100 Degree nodes.\n",
      "  Calculating Top RWR nodes from target...\n",
      "  ✅ Calculated Top 100 RWR nodes from '9606.ENSP00000287295'.\n",
      "\n",
      "  Saving baseline node lists to file...\n",
      "  ✅ Static baseline node lists saved to: simulation_results/baseline_nodes.txt\n",
      "\n",
      "--- Cell 3.1: Static Baseline Calculation and Saving Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.1: Calculate and Save Static Baselines (Degree and RWR)\n",
    "# Description: Calculates the Top Degree and Top Random Walk with Restart (RWR)\n",
    "#              nodes for the prepared AIFM1 graph. Saves these lists to a text file\n",
    "#              (baseline_nodes.txt) in the main simulation_results directory\n",
    "#              for use by analysis notebooks.\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import random # Ensure random is imported for potential seeding\n",
    "import traceback # Ensure traceback is imported\n",
    "\n",
    "print(f\"\\n--- Cell 3.1: Calculate and Save Static Baselines (Degree and RWR) ({time.strftime('%Y-%m-%d %H:%M:%S')}) ---\")\n",
    "\n",
    "# --- Prerequisites Check ---\n",
    "baselines_calc_error = False\n",
    "\n",
    "# Check if graph data is available from Cell 3\n",
    "if 'local_G' not in globals() or not isinstance(local_G, nx.Graph) or local_G.number_of_nodes() == 0:\n",
    "    print(\"❌ Baseline Calculation Error: Graph 'local_G' missing or invalid (Run Cell 3).\")\n",
    "    baselines_calc_error = True\n",
    "if 'local_node_list' not in globals() or not isinstance(local_node_list, list) or not local_node_list:\n",
    "    print(\"❌ Baseline Calculation Error: Node list 'local_node_list' missing or invalid (Run Cell 3).\")\n",
    "    baselines_calc_error = True\n",
    "if 'local_node_to_int' not in globals() or not isinstance(local_node_to_int, dict) or not local_node_to_int:\n",
    "     print(\"❌ Baseline Calculation Error: Node map 'local_node_to_int' missing or invalid (Run Cell 3).\")\n",
    "     baselines_calc_error = True\n",
    "if 'local_INITIAL_SEED_NODES_IDX' not in globals() or not isinstance(local_INITIAL_SEED_NODES_IDX, list):\n",
    "     print(\"❌ Baseline Calculation Error: Seed index list 'local_INITIAL_SEED_NODES_IDX' missing or invalid (Run Cell 3).\")\n",
    "     baselines_calc_error = True\n",
    "\n",
    "# Check if output directory is available from Cell 1 or 2\n",
    "output_dir_base = globals().get('OUTPUT_DIR', 'simulation_results')\n",
    "if not output_dir_base or not os.path.isdir(output_dir_base):\n",
    "     print(f\"❌ Baseline Saving Error: Base output directory '{output_dir_base}' missing or invalid. Check Cell 1 or 2.\")\n",
    "     baselines_calc_error = True\n",
    "\n",
    "\n",
    "# --- Define Output File Path ---\n",
    "# This file is saved in the main simulation_results directory\n",
    "baseline_output_filepath = os.path.join(output_dir_base, \"baseline_nodes.txt\")\n",
    "print(f\"Static baseline node lists will be saved to: {baseline_output_filepath}\")\n",
    "\n",
    "\n",
    "# --- Define Number of Top Nodes to Save ---\n",
    "# Saving a reasonable number (e.g., top 100) allows analysis notebooks to slice as needed.\n",
    "N_TOP_BASELINE_NODES_TO_SAVE = 100 # Save top 100 for each baseline type\n",
    "print(f\"Will calculate and save top {N_TOP_BASELINE_NODES_TO_SAVE} nodes for each baseline.\")\n",
    "\n",
    "\n",
    "# --- Initialize baseline lists ---\n",
    "top_degree_nodes = []\n",
    "top_rwr_nodes = []\n",
    "\n",
    "\n",
    "# --- Execute Baseline Calculation and Saving ---\n",
    "if not baselines_calc_error:\n",
    "    print(\"\\nCalculating static baseline node lists...\")\n",
    "    try:\n",
    "        # --- 1. Calculate Top Degree Nodes ---\n",
    "        print(\"  Calculating Top Degree nodes...\")\n",
    "        try:\n",
    "            degrees = dict(local_G.degree())\n",
    "            # Sort nodes by degree in descending order and get the top N node IDs\n",
    "            # Ensure nodes are in the local_node_list if necessary, though G should contain them\n",
    "            top_degree_nodes = sorted(degrees, key=degrees.get, reverse=True)[:N_TOP_BASELINE_NODES_TO_SAVE]\n",
    "            print(f\"  ✅ Calculated Top {len(top_degree_nodes)} Degree nodes.\")\n",
    "        except Exception as e_deg:\n",
    "            print(f\"  ❌ Error calculating Top Degree nodes: {e_deg}\"); traceback.print_exc(limit=1)\n",
    "\n",
    "\n",
    "        # --- 2. Calculate Top RWR Nodes ---\n",
    "        print(\"  Calculating Top RWR nodes from target...\")\n",
    "        try:\n",
    "            # Need the target node ID to start RWR\n",
    "            if not local_INITIAL_SEED_NODES_IDX:\n",
    "                 warnings.warn(\"⚠️ Cannot calculate RWR baseline: No seed index available in 'local_INITIAL_SEED_NODES_IDX'. Skipping RWR.\")\n",
    "            else:\n",
    "                 # Get ID from the first seed index (assuming seeds mode)\n",
    "                 target_node_id = local_node_list[local_INITIAL_SEED_NODES_IDX[0]]\n",
    "\n",
    "                 if target_node_id not in local_G:\n",
    "                      warnings.warn(f\"⚠️ Cannot calculate RWR baseline: Target node '{target_node_id}' not found in graph 'local_G'. Skipping RWR.\")\n",
    "                 elif local_G.number_of_edges() == 0:\n",
    "                       warnings.warn(\"⚠️ Cannot calculate RWR baseline: Graph 'local_G' has no edges. Skipping RWR.\")\n",
    "                 else:\n",
    "                     # Define personalization vector: 1.0 for the target node, 0.0 for others\n",
    "                     personalization = {node: 0.0 for node in local_G.nodes()}\n",
    "                     personalization[target_node_id] = 1.0\n",
    "\n",
    "                     # Calculate PageRank (RWR) scores\n",
    "                     # REMOVED 'seed' argument from nx.pagerank as it's not standard\n",
    "                     rwr_scores = nx.pagerank(local_G, alpha=0.85, personalization=personalization,\n",
    "                                                weight='weight', max_iter=1000, tol=1.0e-6)\n",
    "\n",
    "                     # Sort nodes by RWR score in descending order and get the top N\n",
    "                     top_rwr_nodes = sorted(rwr_scores, key=rwr_scores.get, reverse=True)[:N_TOP_BASELINE_NODES_TO_SAVE]\n",
    "                     print(f\"  ✅ Calculated Top {len(top_rwr_nodes)} RWR nodes from '{target_node_id}'.\")\n",
    "\n",
    "        except Exception as e_rwr:\n",
    "            print(f\"  ❌ Error calculating Top RWR nodes: {e_rwr}\"); traceback.print_exc(limit=1)\n",
    "\n",
    "\n",
    "        # --- 3. Save Baseline Node Lists to File ---\n",
    "        print(\"\\n  Saving baseline node lists to file...\")\n",
    "        try:\n",
    "            with open(baseline_output_filepath, 'w') as f:\n",
    "                f.write(\"--- Baseline: Top Nodes by Degree ---\\n\")\n",
    "                # Write node IDs, one per line\n",
    "                for node_id in top_degree_nodes:\n",
    "                    f.write(f\"{node_id}\\n\")\n",
    "\n",
    "                f.write(\"\\n--- Baseline: Top Nodes by RWR from Target ---\\n\")\n",
    "                 # Write node IDs, one per line\n",
    "                for node_id in top_rwr_nodes:\n",
    "                    f.write(f\"{node_id}\\n\")\n",
    "\n",
    "                # Add placeholder for Leiden if that analysis is added later\n",
    "                f.write(\"\\n--- Baseline: Target Node Community (Leiden) ---\\n\")\n",
    "                f.write(\"N/A (Leiden baseline not calculated in setup)\\n\") # Explicitly state not included here\n",
    "\n",
    "\n",
    "            print(f\"  ✅ Static baseline node lists saved to: {baseline_output_filepath}\")\n",
    "\n",
    "        except Exception as e_save:\n",
    "            print(f\"  ❌ Error saving static baseline node lists: {e_save}\"); traceback.print_exc(limit=1)\n",
    "\n",
    "    except Exception as e_calc_save_block:\n",
    "        # Catch any error in the main calculation/save block if not caught above\n",
    "        print(f\"❌ An unexpected error occurred during baseline calculation or saving: {e_calc_save_block}\"); traceback.print_exc()\n",
    "\n",
    "else: # baselines_calc_error was True from prereqs\n",
    "    print(\"Skipping static baseline calculation and saving due to missing prerequisites.\")\n",
    "\n",
    "print(\"\\n--- Cell 3.1: Static Baseline Calculation and Saving Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-regression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
